{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is an implementation of Gradient Boosting Machines (GBM) and is used for supervised learning. XGBoost is an open-sourced machine learning library available in Python, R, Julia, Java, C++, Scala. The features that standout are:\n",
    "\n",
    "- Speed\n",
    "- Awareness of sparse data\n",
    "- Implementation on single, distributed systems and out-of-core computation\n",
    "- Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting carries the principle of Gradient Descent and Boosting to supervised learning. Gradient Boosted Models (GBM’s) are trees built sequentially, in series. In GBM’s, we take the weighted sum of multiple models.\n",
    "\n",
    "- Each new model uses Gradient Descent optimization to update/ make corrections to the weights to be learned by the model to reach a local minima of the cost function.\n",
    "- The vector of weights assigned to each model is not derived from the misclassifications of the previous model and the resulting increased weights assigned to misclassifications, but is derived from the weights optimized by Gradient Descent to minimize the cost function. The result of Gradient Descent is the same function of the model as the beginning, just with better parameters.\n",
    "- Gradient Boosting adds a new function to the existing function in each step to predict the output. The result of Gradient Boosting is an altogether different function from the beginning, because the result is the addition of multiple functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "              validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38245219347581555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.5, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 4, \n",
    "                           alpha = 1, \n",
    "                           #scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=1, reg_lambda=1, scale_pos_weight=1, subsample=0.5,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.775785\n",
      "F1: 0.652778\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 3, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=4,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.659772</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.001479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.639740</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.640184</td>\n",
       "      <td>0.010529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.627790</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.629019</td>\n",
       "      <td>0.010380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.614868</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.616503</td>\n",
       "      <td>0.015193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599253</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.601503</td>\n",
       "      <td>0.014880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.378888</td>\n",
       "      <td>0.008461</td>\n",
       "      <td>0.435479</td>\n",
       "      <td>0.036332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.378043</td>\n",
       "      <td>0.008552</td>\n",
       "      <td>0.435516</td>\n",
       "      <td>0.036521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.377614</td>\n",
       "      <td>0.008814</td>\n",
       "      <td>0.435141</td>\n",
       "      <td>0.036482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.377239</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>0.435037</td>\n",
       "      <td>0.036325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.376885</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>0.434845</td>\n",
       "      <td>0.036372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.659772           0.000848           0.660168   \n",
       "1              0.639740           0.006761           0.640184   \n",
       "2              0.627790           0.006071           0.629019   \n",
       "3              0.614868           0.010607           0.616503   \n",
       "4              0.599253           0.010672           0.601503   \n",
       "..                  ...                ...                ...   \n",
       "114            0.378888           0.008461           0.435479   \n",
       "115            0.378043           0.008552           0.435516   \n",
       "116            0.377614           0.008814           0.435141   \n",
       "117            0.377239           0.008746           0.435037   \n",
       "118            0.376885           0.008634           0.434845   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001479  \n",
       "1            0.010529  \n",
       "2            0.010380  \n",
       "3            0.015193  \n",
       "4            0.014880  \n",
       "..                ...  \n",
       "114          0.036332  \n",
       "115          0.036521  \n",
       "116          0.036482  \n",
       "117          0.036325  \n",
       "118          0.036372  \n",
       "\n",
       "[119 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8919\n",
      "AUC Score (Train): 0.939281\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'learning_rate': [0.1,0.07,0.05,0.03,0.01],\n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.5,0.45,0.4],\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = clf_xgb,\n",
    "    param_grid = param_dist, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 675 candidates, totalling 3375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   29.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:   41.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:   56.5s\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3375 out of 3375 | elapsed:  1.3min finished\n",
      "/Users/carlosruiz/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:849: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs...\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs...\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.06076355, 0.15456433, 0.19377222, 0.03920798, 0.12942524,\n",
       "        0.18632941, 0.04027567, 0.11017256, 0.22392254, 0.05056462,\n",
       "        0.1803339 , 0.2590652 , 0.05315404, 0.13909445, 0.2140439 ,\n",
       "        0.04519782, 0.12433543, 0.20956826, 0.05060396, 0.14491487,\n",
       "        0.24640846, 0.0538137 , 0.14658833, 0.23546314, 0.05243211,\n",
       "        0.13976021, 0.23849292, 0.05668225, 0.15903926, 0.27094417,\n",
       "        0.05795164, 0.15846248, 0.26026182, 0.05715942, 0.15704293,\n",
       "        0.25582333, 0.06253262, 0.17746348, 0.30594277, 0.07766094,\n",
       "        0.16767864, 0.28842196, 0.0620326 , 0.16257572, 0.26184602,\n",
       "        0.03861742, 0.10513601, 0.17406268, 0.03791151, 0.10415397,\n",
       "        0.18041286, 0.04046588, 0.10810428, 0.18105121, 0.04708638,\n",
       "        0.13443742, 0.22819991, 0.05001097, 0.13546119, 0.22223043,\n",
       "        0.04769592, 0.13554063, 0.22770624, 0.05980387, 0.15930185,\n",
       "        0.26365252, 0.05821381, 0.16066566, 0.26308522, 0.05791078,\n",
       "        0.15655522, 0.2617898 , 0.06539741, 0.18151293, 0.29349823,\n",
       "        0.06366448, 0.17840199, 0.29648886, 0.06446643, 0.17068534,\n",
       "        0.28630123, 0.0721396 , 0.20330992, 0.32359819, 0.06806488,\n",
       "        0.19483733, 0.32782435, 0.07037807, 0.19888892, 0.33252726,\n",
       "        0.0500526 , 0.12947397, 0.21109533, 0.04744711, 0.12712636,\n",
       "        0.20786319, 0.04584904, 0.13032074, 0.21417103, 0.05554094,\n",
       "        0.14924202, 0.23273277, 0.05287514, 0.13924065, 0.23419552,\n",
       "        0.05176902, 0.14509425, 0.23519621, 0.05508146, 0.15738201,\n",
       "        0.28022132, 0.0586493 , 0.17782903, 0.28281631, 0.05952449,\n",
       "        0.17453818, 0.28442225, 0.06931701, 0.20197868, 0.36883307,\n",
       "        0.08720965, 0.18965139, 0.35246038, 0.08604374, 0.17947502,\n",
       "        0.27753234, 0.06575646, 0.20473437, 0.33443279, 0.06727395,\n",
       "        0.19223671, 0.3489398 , 0.08009758, 0.21862259, 0.33656211,\n",
       "        0.04814487, 0.1255538 , 0.20517278, 0.04714265, 0.12480116,\n",
       "        0.20445991, 0.04552083, 0.12838311, 0.20382543, 0.05046654,\n",
       "        0.14054952, 0.23345017, 0.05182796, 0.14530787, 0.23523297,\n",
       "        0.05127192, 0.14133859, 0.23427024, 0.05907421, 0.16092091,\n",
       "        0.26390162, 0.05949373, 0.15826011, 0.26350627, 0.055583  ,\n",
       "        0.16147156, 0.26510139, 0.06458764, 0.18321409, 0.30126538,\n",
       "        0.06127019, 0.18333697, 0.30358591, 0.0604785 , 0.18491373,\n",
       "        0.34508224, 0.08614831, 0.25028071, 0.40537739, 0.08397427,\n",
       "        0.24732509, 0.36459255, 0.07403765, 0.21048574, 0.33135505,\n",
       "        0.04768801, 0.12755122, 0.21573882, 0.04626918, 0.13271284,\n",
       "        0.21873741, 0.04872785, 0.13572984, 0.21382279, 0.05804024,\n",
       "        0.15378084, 0.25458918, 0.05554357, 0.15685258, 0.24858017,\n",
       "        0.05226355, 0.15139656, 0.25629463, 0.06300325, 0.17513375,\n",
       "        0.29973931, 0.06610074, 0.18992209, 0.31419382, 0.06921749,\n",
       "        0.19058561, 0.31147809, 0.07594395, 0.21721253, 0.34119658,\n",
       "        0.07027082, 0.19774704, 0.3407948 , 0.06809225, 0.20315452,\n",
       "        0.34543676, 0.07878771, 0.22687254, 0.41068897, 0.08038135,\n",
       "        0.25364461, 0.37454796, 0.08101039, 0.21233497, 0.3613378 ,\n",
       "        0.05188212, 0.13649836, 0.21637826, 0.05336227, 0.13758078,\n",
       "        0.20591717, 0.04481349, 0.15316706, 0.24248738, 0.05739994,\n",
       "        0.1618866 , 0.27439666, 0.05719786, 0.17758603, 0.26748352,\n",
       "        0.05581598, 0.15563345, 0.25956812, 0.06289339, 0.17860966,\n",
       "        0.30805736, 0.06450357, 0.16921043, 0.28704667, 0.06481891,\n",
       "        0.17845726, 0.29856734, 0.07464895, 0.18799615, 0.33296776,\n",
       "        0.07287393, 0.19635096, 0.30754204, 0.06914182, 0.18353086,\n",
       "        0.30095739, 0.07287464, 0.20945287, 0.34926424, 0.07336283,\n",
       "        0.20799613, 0.33936596, 0.07334557, 0.19751148, 0.32862053,\n",
       "        0.04708643, 0.12928023, 0.20734067, 0.04662743, 0.13122191,\n",
       "        0.21138787, 0.04755921, 0.12993584, 0.20900826, 0.05428596,\n",
       "        0.15091295, 0.25551243, 0.05687575, 0.15598507, 0.2503264 ,\n",
       "        0.05288739, 0.15233464, 0.25039725, 0.06191578, 0.17487683,\n",
       "        0.29106078, 0.0621911 , 0.17137742, 0.28473215, 0.06095309,\n",
       "        0.1710238 , 0.28050041, 0.06892614, 0.19648299, 0.32086439,\n",
       "        0.06850772, 0.19636202, 0.3166997 , 0.06613975, 0.18070145,\n",
       "        0.31976123, 0.07221837, 0.22628536, 0.36221914, 0.07996778,\n",
       "        0.20945892, 0.34998078, 0.074822  , 0.20696993, 0.3418932 ,\n",
       "        0.04968181, 0.13516197, 0.22319083, 0.04832039, 0.13551641,\n",
       "        0.21810679, 0.05088921, 0.12961249, 0.2235229 , 0.05795932,\n",
       "        0.15974345, 0.25248084, 0.0572823 , 0.152002  , 0.24915838,\n",
       "        0.05474229, 0.15074911, 0.24774613, 0.05975566, 0.17548437,\n",
       "        0.27663083, 0.06778431, 0.17009702, 0.28740396, 0.06368804,\n",
       "        0.17214684, 0.3021801 , 0.06832004, 0.18180294, 0.30616722,\n",
       "        0.07375684, 0.1936254 , 0.31427789, 0.06641226, 0.18619933,\n",
       "        0.30763297, 0.07768378, 0.20986261, 0.34764476, 0.07488394,\n",
       "        0.2062326 , 0.3417398 , 0.07164464, 0.20189314, 0.31634369,\n",
       "        0.05183005, 0.13415971, 0.21245966, 0.05228391, 0.13680062,\n",
       "        0.22684698, 0.05041552, 0.1353025 , 0.21772194, 0.05746264,\n",
       "        0.15849099, 0.26016216, 0.05675879, 0.16475711, 0.25289893,\n",
       "        0.05557055, 0.15749378, 0.2578084 , 0.06492858, 0.18007469,\n",
       "        0.30198026, 0.06481738, 0.18372712, 0.30746765, 0.06968808,\n",
       "        0.18079224, 0.31223574, 0.06964817, 0.18802991, 0.32563429,\n",
       "        0.06670303, 0.195857  , 0.33088608, 0.0647501 , 0.18378692,\n",
       "        0.30959854, 0.07852421, 0.21379485, 0.35521364, 0.07273502,\n",
       "        0.20639858, 0.34456186, 0.07254262, 0.19372306, 0.33021398,\n",
       "        0.04621363, 0.13136525, 0.21833587, 0.04786696, 0.13340597,\n",
       "        0.21611018, 0.04891386, 0.13252463, 0.21918635, 0.05192375,\n",
       "        0.15386291, 0.25143309, 0.05960045, 0.15536351, 0.2594461 ,\n",
       "        0.05753946, 0.15819855, 0.2681479 , 0.06591759, 0.18863959,\n",
       "        0.30073562, 0.06497593, 0.18342791, 0.30630879, 0.0633698 ,\n",
       "        0.18349423, 0.29908381, 0.07115502, 0.2003747 , 0.33924885,\n",
       "        0.07019663, 0.20027022, 0.33775864, 0.06871791, 0.20857162,\n",
       "        0.33149014, 0.08007703, 0.22084217, 0.3670958 , 0.07635546,\n",
       "        0.2037159 , 0.35665965, 0.07620158, 0.20233283, 0.33669381,\n",
       "        0.04513283, 0.11936064, 0.20043073, 0.04202604, 0.12294159,\n",
       "        0.20250354, 0.04516664, 0.12434106, 0.19951458, 0.05157781,\n",
       "        0.15200772, 0.23162999, 0.0490386 , 0.14235115, 0.22022557,\n",
       "        0.05216713, 0.13718429, 0.22580285, 0.05775771, 0.15354519,\n",
       "        0.25424542, 0.05383964, 0.15345359, 0.25565476, 0.05482721,\n",
       "        0.15357265, 0.25902562, 0.06368766, 0.17454772, 0.29029884,\n",
       "        0.06473289, 0.17172637, 0.27794838, 0.0602704 , 0.1718585 ,\n",
       "        0.27753091, 0.06898184, 0.1834322 , 0.3027328 , 0.06703472,\n",
       "        0.18312721, 0.29546509, 0.06339526, 0.17193565, 0.28908772,\n",
       "        0.04542832, 0.12460623, 0.20386782, 0.04296026, 0.12731462,\n",
       "        0.2074213 , 0.04485207, 0.12425137, 0.20740204, 0.05184784,\n",
       "        0.1426393 , 0.23138423, 0.04934497, 0.14222836, 0.22882905,\n",
       "        0.05246649, 0.13708582, 0.23139334, 0.05514317, 0.16572642,\n",
       "        0.25788097, 0.054772  , 0.15139823, 0.25200629, 0.05437422,\n",
       "        0.14100347, 0.24790225, 0.06118817, 0.169836  , 0.27571402,\n",
       "        0.05750618, 0.16358438, 0.27808595, 0.06810322, 0.1581974 ,\n",
       "        0.26781616, 0.06709275, 0.1749608 , 0.30013833, 0.06417365,\n",
       "        0.18556199, 0.29260898, 0.06126785, 0.17668133, 0.28571539,\n",
       "        0.0479773 , 0.12267981, 0.20269051, 0.04614081, 0.12742057,\n",
       "        0.20513091, 0.0480135 , 0.12388244, 0.2055295 , 0.05233955,\n",
       "        0.14915528, 0.2413805 , 0.05322323, 0.14732108, 0.2331192 ,\n",
       "        0.05192976, 0.13784323, 0.24071217, 0.05815244, 0.1601058 ,\n",
       "        0.27086391, 0.05955706, 0.16125402, 0.26147785, 0.05481   ,\n",
       "        0.15451937, 0.25924649, 0.06098266, 0.16446948, 0.27541881,\n",
       "        0.06081777, 0.16161041, 0.27116337, 0.056106  , 0.15889583,\n",
       "        0.267663  , 0.06394   , 0.17610722, 0.29399729, 0.06372166,\n",
       "        0.17910752, 0.29543223, 0.06418209, 0.16809363, 0.27741089,\n",
       "        0.0440208 , 0.11839728, 0.20534348, 0.0420311 , 0.12036128,\n",
       "        0.19804177, 0.0458096 , 0.12077975, 0.19913559, 0.05254121,\n",
       "        0.14399562, 0.23892574, 0.05193148, 0.14417939, 0.25454721,\n",
       "        0.05120625, 0.13300962, 0.23055344, 0.0644712 , 0.16602097,\n",
       "        0.26712217, 0.05844579, 0.15910621, 0.26671162, 0.0572927 ,\n",
       "        0.16012306, 0.2585032 , 0.06100988, 0.17661252, 0.28828077,\n",
       "        0.06386023, 0.17208133, 0.28822279, 0.061235  , 0.16295037,\n",
       "        0.28490472, 0.06307058, 0.18706441, 0.31448226, 0.06728549,\n",
       "        0.18695464, 0.29430799, 0.06579132, 0.17967935, 0.28917031,\n",
       "        0.04321566, 0.11301842, 0.18814354, 0.04389043, 0.12922626,\n",
       "        0.19169903, 0.04472766, 0.11940169, 0.20182438, 0.04852381,\n",
       "        0.14026837, 0.22531314, 0.05033937, 0.13926368, 0.22649784,\n",
       "        0.05127282, 0.13583961, 0.23289061, 0.05520763, 0.15766878,\n",
       "        0.25577021, 0.05697718, 0.1553596 , 0.25530553, 0.05345845,\n",
       "        0.15132813, 0.25811067, 0.06197362, 0.1745254 , 0.2902205 ,\n",
       "        0.06346745, 0.17294378, 0.28631382, 0.06203861, 0.16530333,\n",
       "        0.27610979, 0.06885777, 0.18723302, 0.31228409, 0.06471868,\n",
       "        0.18018765, 0.30194807, 0.06295981, 0.1708293 , 0.22780166]),\n",
       " 'std_fit_time': array([0.01335054, 0.02210487, 0.003906  , 0.00192886, 0.00591159,\n",
       "        0.00691902, 0.00269625, 0.00318877, 0.02663249, 0.00389096,\n",
       "        0.00979296, 0.01518319, 0.00517996, 0.00265779, 0.00349651,\n",
       "        0.00191938, 0.00077233, 0.00881218, 0.00072209, 0.00207287,\n",
       "        0.00471904, 0.0028154 , 0.00240714, 0.00385201, 0.0026629 ,\n",
       "        0.0040375 , 0.01947625, 0.00271496, 0.00983381, 0.00486856,\n",
       "        0.0020281 , 0.00332177, 0.00249676, 0.00123491, 0.00458397,\n",
       "        0.0091632 , 0.00294018, 0.00244769, 0.02075398, 0.00903227,\n",
       "        0.00955779, 0.00678228, 0.00511975, 0.00265913, 0.00798103,\n",
       "        0.00218712, 0.00307086, 0.00656359, 0.00105627, 0.00359418,\n",
       "        0.00705147, 0.0017581 , 0.0019009 , 0.00253151, 0.00112434,\n",
       "        0.00269814, 0.0246948 , 0.00232821, 0.00468671, 0.00592482,\n",
       "        0.00058642, 0.01267519, 0.02143855, 0.00710927, 0.00687563,\n",
       "        0.00632998, 0.00096488, 0.00676347, 0.00519267, 0.00321556,\n",
       "        0.0020053 , 0.00814446, 0.00281526, 0.00467721, 0.00688784,\n",
       "        0.00194803, 0.00367122, 0.0085776 , 0.00387553, 0.00383797,\n",
       "        0.00465185, 0.00322941, 0.00977786, 0.00454486, 0.00085526,\n",
       "        0.00636829, 0.00310628, 0.0033078 , 0.00432932, 0.00581057,\n",
       "        0.00448415, 0.00495385, 0.0049143 , 0.00551688, 0.00584926,\n",
       "        0.0064251 , 0.00234777, 0.00873649, 0.00184777, 0.00262546,\n",
       "        0.00520287, 0.00750827, 0.0023395 , 0.00459017, 0.01172249,\n",
       "        0.00429261, 0.00438941, 0.03470905, 0.00172395, 0.00224339,\n",
       "        0.00746082, 0.00159669, 0.0060791 , 0.00581233, 0.00563523,\n",
       "        0.0026535 , 0.00801757, 0.00423592, 0.00279979, 0.0029446 ,\n",
       "        0.01147676, 0.00337629, 0.00469556, 0.00526496, 0.005353  ,\n",
       "        0.00591856, 0.0013471 , 0.00802324, 0.00447245, 0.00183756,\n",
       "        0.00663777, 0.00494446, 0.01195938, 0.00318121, 0.01233782,\n",
       "        0.00619717, 0.00489379, 0.00688243, 0.00330287, 0.00242539,\n",
       "        0.00348923, 0.00293075, 0.00460469, 0.00617745, 0.00252522,\n",
       "        0.00143022, 0.00551383, 0.00297651, 0.00356987, 0.00541314,\n",
       "        0.00224772, 0.0049964 , 0.00238308, 0.00197836, 0.00420412,\n",
       "        0.00227182, 0.00482715, 0.00782836, 0.00461744, 0.00276531,\n",
       "        0.00392981, 0.00245768, 0.0013626 , 0.00444843, 0.00565304,\n",
       "        0.00126372, 0.00469297, 0.005972  , 0.00244641, 0.00830064,\n",
       "        0.01998585, 0.00307488, 0.00462204, 0.00989382, 0.0054359 ,\n",
       "        0.00792545, 0.0064585 , 0.00352444, 0.00476541, 0.00466351,\n",
       "        0.00349862, 0.001995  , 0.00103583, 0.00334811, 0.00478375,\n",
       "        0.00306141, 0.00405011, 0.00164391, 0.0011014 , 0.00415316,\n",
       "        0.00409935, 0.00475162, 0.00248546, 0.01058954, 0.01171916,\n",
       "        0.00290267, 0.0021718 , 0.0141019 , 0.00341004, 0.01485986,\n",
       "        0.02214328, 0.00251597, 0.01751095, 0.00219438, 0.0022298 ,\n",
       "        0.00289555, 0.00510569, 0.00464756, 0.00652688, 0.00154914,\n",
       "        0.0025679 , 0.00164421, 0.01406459, 0.00709687, 0.01074718,\n",
       "        0.00420576, 0.00193283, 0.00837665, 0.01498518, 0.0062316 ,\n",
       "        0.01159069, 0.01398047, 0.00676952, 0.00425591, 0.0025697 ,\n",
       "        0.00085832, 0.00406238, 0.00975049, 0.02260333, 0.03837103,\n",
       "        0.00859976, 0.00337512, 0.0093847 , 0.00879522, 0.00289236,\n",
       "        0.00397111, 0.0086412 , 0.00275589, 0.01918688, 0.02231796,\n",
       "        0.00122395, 0.01040284, 0.00401447, 0.00343097, 0.01228765,\n",
       "        0.03264382, 0.00347836, 0.0048529 , 0.00520968, 0.00452717,\n",
       "        0.00326783, 0.00642748, 0.00489664, 0.00570782, 0.01743557,\n",
       "        0.00566331, 0.00981775, 0.0110727 , 0.00193259, 0.00460241,\n",
       "        0.0026383 , 0.00309762, 0.00181559, 0.01696892, 0.00283055,\n",
       "        0.00660674, 0.0036762 , 0.00426848, 0.00414988, 0.00803379,\n",
       "        0.00210629, 0.00768455, 0.00630728, 0.00249956, 0.00936231,\n",
       "        0.00455862, 0.00181825, 0.00334143, 0.00418352, 0.00381747,\n",
       "        0.00569041, 0.00652114, 0.0035099 , 0.00505975, 0.00533498,\n",
       "        0.00310659, 0.00437544, 0.00293511, 0.00260284, 0.00423702,\n",
       "        0.00327154, 0.00344988, 0.00377002, 0.00624339, 0.00323111,\n",
       "        0.00759625, 0.00637136, 0.00257173, 0.00195199, 0.00368542,\n",
       "        0.00316926, 0.00477531, 0.00625266, 0.00308668, 0.00370658,\n",
       "        0.04132122, 0.00115511, 0.00469758, 0.00885639, 0.0051863 ,\n",
       "        0.0044029 , 0.00258945, 0.00321998, 0.00218028, 0.01052608,\n",
       "        0.00250245, 0.01081438, 0.00792784, 0.00289733, 0.00072581,\n",
       "        0.02230309, 0.00817272, 0.00421978, 0.00287507, 0.00438508,\n",
       "        0.00339229, 0.00538462, 0.00250065, 0.00407181, 0.00351314,\n",
       "        0.00149695, 0.00720068, 0.02002213, 0.00431127, 0.00387717,\n",
       "        0.00566678, 0.01363811, 0.00323423, 0.00392945, 0.00261735,\n",
       "        0.00316271, 0.0510378 , 0.00260961, 0.00610385, 0.00411402,\n",
       "        0.01907123, 0.00460055, 0.00523077, 0.00301083, 0.00356549,\n",
       "        0.00481329, 0.0036862 , 0.00492239, 0.00389315, 0.00411123,\n",
       "        0.01505391, 0.01610176, 0.00398952, 0.00193816, 0.01420545,\n",
       "        0.01038835, 0.02507696, 0.00390282, 0.0023042 , 0.00381581,\n",
       "        0.00352178, 0.00145932, 0.00395825, 0.00402227, 0.00335915,\n",
       "        0.00421096, 0.00227787, 0.00426872, 0.01322643, 0.00086438,\n",
       "        0.00725695, 0.00508505, 0.0032345 , 0.00337394, 0.00206617,\n",
       "        0.01124035, 0.00119029, 0.00599704, 0.00572805, 0.00268826,\n",
       "        0.00426886, 0.06097451, 0.00285006, 0.00464526, 0.00441486,\n",
       "        0.00263252, 0.00522659, 0.03514241, 0.00238353, 0.00732327,\n",
       "        0.0021183 , 0.00217095, 0.00320371, 0.00287319, 0.00288522,\n",
       "        0.00679537, 0.00377528, 0.00436757, 0.00880615, 0.00251743,\n",
       "        0.0040612 , 0.00403861, 0.00304121, 0.00231507, 0.00337022,\n",
       "        0.0064806 , 0.00199807, 0.00387973, 0.00610932, 0.00220002,\n",
       "        0.00375226, 0.00380028, 0.01015733, 0.01005062, 0.00160042,\n",
       "        0.00414944, 0.00368518, 0.00388803, 0.00289743, 0.00542526,\n",
       "        0.00414117, 0.00332398, 0.0023366 , 0.00860798, 0.00149589,\n",
       "        0.00582723, 0.01518745, 0.00251622, 0.0135329 , 0.00823536,\n",
       "        0.0041397 , 0.00465862, 0.00524727, 0.00115003, 0.00576145,\n",
       "        0.00512892, 0.00385467, 0.00823732, 0.00472146, 0.00303037,\n",
       "        0.00441773, 0.0061766 , 0.00476954, 0.00529391, 0.01906797,\n",
       "        0.00199678, 0.00872921, 0.01277348, 0.00418956, 0.00541318,\n",
       "        0.00295478, 0.00390155, 0.00321629, 0.00429946, 0.00548311,\n",
       "        0.01515898, 0.01804444, 0.00173696, 0.00338901, 0.01639413,\n",
       "        0.00410151, 0.00297273, 0.00403072, 0.00546475, 0.00254175,\n",
       "        0.00310433, 0.00066111, 0.0021976 , 0.00651411, 0.00198068,\n",
       "        0.0019767 , 0.0067966 , 0.00413867, 0.00546307, 0.00246792,\n",
       "        0.00331721, 0.01269411, 0.005472  , 0.00204568, 0.00465148,\n",
       "        0.00335547, 0.00491379, 0.00262164, 0.01279896, 0.00512678,\n",
       "        0.00316628, 0.00633421, 0.00116799, 0.00450209, 0.01425105,\n",
       "        0.00460378, 0.00503902, 0.00400187, 0.00206495, 0.00682139,\n",
       "        0.0021133 , 0.00272468, 0.00261716, 0.00467244, 0.00287611,\n",
       "        0.00253332, 0.00525684, 0.00355351, 0.00301868, 0.00547283,\n",
       "        0.00325533, 0.00139496, 0.0046254 , 0.00420865, 0.00800506,\n",
       "        0.00604686, 0.00324916, 0.00830989, 0.00886737, 0.00191637,\n",
       "        0.00816115, 0.00938416, 0.00392172, 0.0049361 , 0.00403294,\n",
       "        0.00324988, 0.00391483, 0.00338618, 0.00918973, 0.00671459,\n",
       "        0.01981412, 0.00293706, 0.00873572, 0.00448865, 0.00316316,\n",
       "        0.00647328, 0.0055183 , 0.00144089, 0.01049857, 0.02479788,\n",
       "        0.00678658, 0.00170548, 0.00481167, 0.00360947, 0.00258147,\n",
       "        0.00551943, 0.00141634, 0.00485884, 0.00553106, 0.0036336 ,\n",
       "        0.00480812, 0.00318791, 0.00340303, 0.00298408, 0.00505884,\n",
       "        0.00172598, 0.00933469, 0.00580981, 0.0063925 , 0.01078898,\n",
       "        0.00483761, 0.00320235, 0.00327582, 0.00602663, 0.0015681 ,\n",
       "        0.00338058, 0.01979608, 0.00566245, 0.00504684, 0.00082963,\n",
       "        0.00202595, 0.00281288, 0.01960445, 0.00210017, 0.0103152 ,\n",
       "        0.0064156 , 0.00452399, 0.00545431, 0.00570786, 0.0034785 ,\n",
       "        0.00569618, 0.0049842 , 0.00299171, 0.01098872, 0.00455656,\n",
       "        0.00326612, 0.00462175, 0.03241918, 0.00229368, 0.00522576,\n",
       "        0.00436222, 0.00307808, 0.00310117, 0.00230229, 0.00287911,\n",
       "        0.00434922, 0.00788201, 0.00275844, 0.00189666, 0.02643176,\n",
       "        0.00158286, 0.00091497, 0.00761686, 0.01040735, 0.0030029 ,\n",
       "        0.00462517, 0.0040193 , 0.00494847, 0.00614469, 0.00238171,\n",
       "        0.00508926, 0.00221892, 0.00278822, 0.00625956, 0.00405094,\n",
       "        0.00334822, 0.00236915, 0.0137903 , 0.00408545, 0.00348748,\n",
       "        0.03616842, 0.00357198, 0.0050965 , 0.00328177, 0.00260431,\n",
       "        0.00212302, 0.01139698, 0.00309844, 0.02014181, 0.02648243,\n",
       "        0.00286465, 0.0027225 , 0.00270001, 0.00496319, 0.01361144,\n",
       "        0.00566934, 0.00268226, 0.00629042, 0.01304075, 0.0039296 ,\n",
       "        0.00590929, 0.00812636, 0.00304644, 0.00329984, 0.00352636,\n",
       "        0.00214219, 0.00210998, 0.00992053, 0.00350327, 0.0047021 ,\n",
       "        0.00594848, 0.00245344, 0.00668619, 0.00467632, 0.00515802,\n",
       "        0.00502024, 0.00360351, 0.00494574, 0.00491844, 0.00485022,\n",
       "        0.00319555, 0.00432388, 0.00376458, 0.00213078, 0.01074749,\n",
       "        0.00902992, 0.00377799, 0.00325359, 0.00522312, 0.00285215,\n",
       "        0.00196997, 0.00400487, 0.00169511, 0.00550611, 0.01291408]),\n",
       " 'mean_score_time': array([0.00687518, 0.00527973, 0.00748906, 0.00321126, 0.00556145,\n",
       "        0.0058332 , 0.00319214, 0.00508437, 0.00863342, 0.00380139,\n",
       "        0.00809107, 0.00631957, 0.00419989, 0.00490837, 0.00580258,\n",
       "        0.00376329, 0.00452437, 0.00625219, 0.00352149, 0.00585923,\n",
       "        0.00688534, 0.00367508, 0.00521402, 0.00613351, 0.003403  ,\n",
       "        0.00471454, 0.00644321, 0.00453587, 0.00606213, 0.00755506,\n",
       "        0.0035728 , 0.00540843, 0.00681491, 0.00378976, 0.00507054,\n",
       "        0.00618491, 0.00412455, 0.00634594, 0.00984721, 0.00462747,\n",
       "        0.00525994, 0.00736494, 0.00387897, 0.00552316, 0.00702481,\n",
       "        0.00310545, 0.00431643, 0.00616894, 0.00322609, 0.00542278,\n",
       "        0.00707936, 0.00324354, 0.0050458 , 0.00564914, 0.00341263,\n",
       "        0.00513573, 0.00607104, 0.0042027 , 0.00480742, 0.00647793,\n",
       "        0.00360713, 0.00521197, 0.00679436, 0.00410352, 0.00568357,\n",
       "        0.00750008, 0.00384059, 0.00605826, 0.007376  , 0.0039206 ,\n",
       "        0.00552635, 0.00702348, 0.00471101, 0.00607762, 0.00735292,\n",
       "        0.00549407, 0.00613461, 0.0079742 , 0.00439992, 0.00623007,\n",
       "        0.00784116, 0.00408239, 0.00640893, 0.00909061, 0.00413284,\n",
       "        0.00605927, 0.00824456, 0.00405421, 0.00596437, 0.00748758,\n",
       "        0.0044034 , 0.00521307, 0.00637007, 0.00495358, 0.00557137,\n",
       "        0.00574393, 0.00458937, 0.00556021, 0.00676107, 0.00394359,\n",
       "        0.00533423, 0.00672126, 0.00506516, 0.00563154, 0.00659232,\n",
       "        0.00420918, 0.00538778, 0.00577726, 0.00392518, 0.0057014 ,\n",
       "        0.00715976, 0.0037889 , 0.0059442 , 0.00993805, 0.00544395,\n",
       "        0.00586138, 0.00706797, 0.00522199, 0.00649352, 0.00759006,\n",
       "        0.00879464, 0.00956874, 0.00815492, 0.00727506, 0.00642557,\n",
       "        0.0077529 , 0.00405354, 0.00757585, 0.00875826, 0.00393791,\n",
       "        0.00668941, 0.01321783, 0.00521369, 0.0066968 , 0.00870652,\n",
       "        0.00353279, 0.00470076, 0.00608778, 0.00413251, 0.00498424,\n",
       "        0.00638795, 0.00450015, 0.00468178, 0.00578756, 0.00478835,\n",
       "        0.00496702, 0.00709801, 0.00457468, 0.00514755, 0.0067615 ,\n",
       "        0.00412984, 0.00571451, 0.00598202, 0.00393462, 0.00542874,\n",
       "        0.00811052, 0.00392022, 0.00517812, 0.00769448, 0.00412073,\n",
       "        0.00551395, 0.00785351, 0.00396113, 0.00584354, 0.00727143,\n",
       "        0.00657101, 0.00574775, 0.0074791 , 0.00429454, 0.00616026,\n",
       "        0.0147716 , 0.00673194, 0.00716929, 0.0105927 , 0.00508032,\n",
       "        0.00611367, 0.0098237 , 0.00407181, 0.0057591 , 0.00811596,\n",
       "        0.0045289 , 0.00492115, 0.00588822, 0.00401144, 0.00552998,\n",
       "        0.0070724 , 0.00403481, 0.00501108, 0.0079895 , 0.00501842,\n",
       "        0.00697594, 0.00795975, 0.00492883, 0.00574617, 0.00693803,\n",
       "        0.00508456, 0.00589299, 0.00648961, 0.0047555 , 0.00700278,\n",
       "        0.00751061, 0.00455213, 0.00581264, 0.00815296, 0.00421934,\n",
       "        0.00661201, 0.00952711, 0.00519009, 0.00707417, 0.01040463,\n",
       "        0.0049335 , 0.00736971, 0.00943637, 0.0050405 , 0.00718598,\n",
       "        0.00903454, 0.00589528, 0.01043153, 0.00942621, 0.00410066,\n",
       "        0.00640397, 0.00963316, 0.00451236, 0.00684123, 0.00862722,\n",
       "        0.00365329, 0.00533171, 0.00644536, 0.00409174, 0.00539136,\n",
       "        0.00722079, 0.00405183, 0.00651875, 0.00825114, 0.00596237,\n",
       "        0.00556822, 0.01263952, 0.00521812, 0.00793648, 0.00706506,\n",
       "        0.00481238, 0.00703521, 0.009654  , 0.00509219, 0.00690293,\n",
       "        0.00763597, 0.0043148 , 0.00569816, 0.00816498, 0.0041995 ,\n",
       "        0.00726695, 0.00960464, 0.00419197, 0.00701313, 0.00906587,\n",
       "        0.00526924, 0.00600882, 0.00762653, 0.00430112, 0.00677905,\n",
       "        0.00896735, 0.00548191, 0.00735693, 0.00834761, 0.00424128,\n",
       "        0.00592351, 0.00887556, 0.00429683, 0.00713921, 0.00776277,\n",
       "        0.00363851, 0.00467973, 0.00651517, 0.00373325, 0.00533128,\n",
       "        0.00547366, 0.00383601, 0.00642495, 0.0074554 , 0.0047296 ,\n",
       "        0.00516658, 0.00665421, 0.00397038, 0.00603499, 0.00633006,\n",
       "        0.00472078, 0.00737219, 0.0060111 , 0.00451851, 0.00628138,\n",
       "        0.00764189, 0.00391884, 0.00647659, 0.0079052 , 0.00563512,\n",
       "        0.00646825, 0.00910878, 0.00496187, 0.0065587 , 0.00759482,\n",
       "        0.00434747, 0.00771246, 0.00774307, 0.00470052, 0.00561781,\n",
       "        0.00854635, 0.0047708 , 0.00696201, 0.00971165, 0.00516701,\n",
       "        0.00645247, 0.01020784, 0.00422897, 0.00673499, 0.00843558,\n",
       "        0.00412378, 0.00554404, 0.00684581, 0.00516787, 0.00589972,\n",
       "        0.00659418, 0.0046083 , 0.00525966, 0.00638409, 0.00581007,\n",
       "        0.00662465, 0.00738597, 0.00572071, 0.0054914 , 0.00691528,\n",
       "        0.00375681, 0.00594039, 0.00715618, 0.00437984, 0.00782166,\n",
       "        0.00776701, 0.00722737, 0.00604544, 0.0069315 , 0.00394101,\n",
       "        0.00612898, 0.00893316, 0.00400958, 0.00625005, 0.00786066,\n",
       "        0.00496778, 0.00624576, 0.01007762, 0.00553513, 0.00671844,\n",
       "        0.00781579, 0.00588613, 0.00778217, 0.00983295, 0.00419917,\n",
       "        0.00674119, 0.00943398, 0.00426536, 0.0073041 , 0.00962567,\n",
       "        0.00518069, 0.00686483, 0.00573134, 0.00393872, 0.00482264,\n",
       "        0.00610504, 0.00473404, 0.00557442, 0.00636191, 0.00535641,\n",
       "        0.00625973, 0.00847926, 0.00443163, 0.00654659, 0.00698166,\n",
       "        0.00499721, 0.00578761, 0.00728345, 0.00528979, 0.0068007 ,\n",
       "        0.00814114, 0.00456004, 0.00632925, 0.00780182, 0.00439372,\n",
       "        0.00608201, 0.00673184, 0.00413198, 0.00749726, 0.00969262,\n",
       "        0.00540943, 0.00598841, 0.00874543, 0.00437961, 0.00604668,\n",
       "        0.00865111, 0.00481873, 0.00753975, 0.00922217, 0.00475211,\n",
       "        0.00669899, 0.00943546, 0.00445642, 0.00605597, 0.00856104,\n",
       "        0.00528479, 0.00705986, 0.00581546, 0.00535231, 0.00482659,\n",
       "        0.00735564, 0.00405512, 0.00506649, 0.00685358, 0.00367842,\n",
       "        0.00549655, 0.00691886, 0.00466013, 0.00547833, 0.00960298,\n",
       "        0.00403013, 0.006708  , 0.00776081, 0.00565228, 0.00575557,\n",
       "        0.00817246, 0.00415201, 0.00555778, 0.00719805, 0.0049171 ,\n",
       "        0.00716305, 0.01207376, 0.00416436, 0.00660405, 0.00923038,\n",
       "        0.00476937, 0.00592093, 0.00898118, 0.00476961, 0.00673709,\n",
       "        0.007581  , 0.00439997, 0.00661106, 0.00859857, 0.00495415,\n",
       "        0.00770297, 0.00943317, 0.00443239, 0.00662594, 0.00744615,\n",
       "        0.00499029, 0.00479112, 0.00621305, 0.00546122, 0.0056354 ,\n",
       "        0.00692739, 0.00545163, 0.00544634, 0.00682855, 0.00437045,\n",
       "        0.00529852, 0.00695195, 0.00562716, 0.00719161, 0.00771003,\n",
       "        0.0039124 , 0.00557928, 0.00677595, 0.00436788, 0.00589085,\n",
       "        0.00677676, 0.00486012, 0.007373  , 0.00902977, 0.00483108,\n",
       "        0.00722413, 0.00870595, 0.00455408, 0.00754323, 0.00855627,\n",
       "        0.00469933, 0.0059186 , 0.00731626, 0.00434203, 0.00640068,\n",
       "        0.00804811, 0.00489407, 0.00860357, 0.00910163, 0.00550017,\n",
       "        0.00606179, 0.01073112, 0.00465865, 0.00999684, 0.00836535,\n",
       "        0.00483494, 0.00579844, 0.00616503, 0.00386233, 0.0061265 ,\n",
       "        0.00666189, 0.00392718, 0.00669408, 0.00654263, 0.00455194,\n",
       "        0.00494781, 0.0084054 , 0.00393801, 0.00593181, 0.00709515,\n",
       "        0.00395851, 0.00745034, 0.0076817 , 0.00434918, 0.00543685,\n",
       "        0.0098886 , 0.00483727, 0.00696521, 0.00891142, 0.00450315,\n",
       "        0.00639515, 0.00806432, 0.00410624, 0.00679803, 0.00911899,\n",
       "        0.00411224, 0.00589361, 0.00773988, 0.0045434 , 0.00714688,\n",
       "        0.00738993, 0.00546818, 0.00692325, 0.00903468, 0.00439482,\n",
       "        0.00585008, 0.00814185, 0.00481873, 0.00811505, 0.00708818,\n",
       "        0.00423045, 0.00579839, 0.00741348, 0.00463195, 0.00620832,\n",
       "        0.00579648, 0.00498466, 0.00584893, 0.00706296, 0.00417123,\n",
       "        0.0052382 , 0.00764799, 0.00393367, 0.0060236 , 0.00777707,\n",
       "        0.0043335 , 0.00523057, 0.00754042, 0.0044343 , 0.00571628,\n",
       "        0.00922284, 0.00482793, 0.0058641 , 0.00863056, 0.005088  ,\n",
       "        0.00566363, 0.00800109, 0.00424814, 0.0062717 , 0.01022682,\n",
       "        0.00441017, 0.00556931, 0.00780902, 0.00482845, 0.00638237,\n",
       "        0.00762305, 0.00427432, 0.00678124, 0.00815253, 0.00423503,\n",
       "        0.00792227, 0.00880165, 0.00395632, 0.00589309, 0.00785427,\n",
       "        0.00450935, 0.00498381, 0.00692315, 0.00453939, 0.00492511,\n",
       "        0.00569334, 0.00390096, 0.00487785, 0.00586824, 0.00454946,\n",
       "        0.005972  , 0.00778041, 0.00415936, 0.00615659, 0.00858197,\n",
       "        0.00410819, 0.00539231, 0.00872159, 0.00492716, 0.00765605,\n",
       "        0.0084919 , 0.00441804, 0.00742893, 0.007758  , 0.00442305,\n",
       "        0.00616159, 0.00839167, 0.00497355, 0.00662584, 0.00807118,\n",
       "        0.00396585, 0.00666251, 0.01022921, 0.00433121, 0.00573416,\n",
       "        0.00748754, 0.00433316, 0.00694594, 0.00987296, 0.00445189,\n",
       "        0.00636268, 0.00856118, 0.0042068 , 0.00607376, 0.00917778,\n",
       "        0.00508294, 0.00513859, 0.00661564, 0.00457311, 0.00613551,\n",
       "        0.00697179, 0.00600262, 0.00544991, 0.00734954, 0.00383043,\n",
       "        0.00561137, 0.00754657, 0.00539532, 0.00686913, 0.0077064 ,\n",
       "        0.00464749, 0.00586805, 0.006427  , 0.00494814, 0.00595717,\n",
       "        0.00830717, 0.00567584, 0.00562372, 0.00696898, 0.00589857,\n",
       "        0.0055336 , 0.00810919, 0.00495939, 0.00618505, 0.00951223,\n",
       "        0.00411081, 0.00813379, 0.00831847, 0.00401745, 0.00685897,\n",
       "        0.00898523, 0.00420794, 0.00630155, 0.00799932, 0.00410194,\n",
       "        0.00806231, 0.00861654, 0.00398922, 0.00622048, 0.00605545]),\n",
       " 'std_score_time': array([5.31594261e-03, 1.14867902e-03, 3.65560013e-03, 1.44617329e-04,\n",
       "        1.38653532e-03, 1.40911003e-03, 2.39577879e-04, 1.28532429e-03,\n",
       "        3.77561227e-03, 4.94370485e-04, 2.80941784e-03, 1.29981177e-03,\n",
       "        1.60439283e-03, 6.11535344e-04, 8.01766955e-04, 4.52626193e-04,\n",
       "        2.12636045e-04, 1.20889140e-03, 2.18768471e-04, 1.18330732e-03,\n",
       "        1.12810152e-03, 4.01366262e-04, 5.95294046e-04, 2.31254489e-04,\n",
       "        8.44563820e-05, 1.95845235e-04, 5.66046568e-04, 1.57168828e-03,\n",
       "        1.40717438e-03, 1.00237247e-03, 1.88171785e-04, 5.77824721e-04,\n",
       "        4.29247539e-04, 4.65474517e-04, 6.75421153e-04, 2.42215516e-04,\n",
       "        6.42244897e-04, 1.89310970e-03, 2.60279937e-03, 1.49080232e-03,\n",
       "        1.45838944e-04, 9.47276355e-04, 5.75107060e-04, 7.01679470e-04,\n",
       "        1.01977244e-03, 9.31253814e-05, 7.66923080e-04, 1.41612491e-03,\n",
       "        1.72686588e-04, 1.68235451e-03, 1.94282104e-03, 2.67597003e-05,\n",
       "        8.85518592e-04, 1.14224540e-03, 1.47287473e-04, 1.04584690e-03,\n",
       "        1.83139825e-04, 4.15400395e-04, 3.17481618e-04, 8.83405159e-04,\n",
       "        1.25244945e-04, 8.05980946e-04, 1.36397565e-03, 2.13786184e-04,\n",
       "        2.75392371e-04, 1.27178219e-03, 2.01827184e-04, 1.07674786e-03,\n",
       "        1.05671893e-03, 3.46209010e-04, 3.86566752e-04, 4.48365662e-04,\n",
       "        1.14909475e-03, 5.39233248e-04, 1.67416474e-04, 1.58649135e-03,\n",
       "        4.27965363e-04, 1.16254592e-03, 6.34919171e-04, 5.95471265e-04,\n",
       "        7.88797431e-04, 1.38240791e-04, 3.52468629e-04, 2.12175441e-03,\n",
       "        2.72056372e-04, 3.61120797e-04, 1.21874853e-03, 3.14777201e-04,\n",
       "        5.46803987e-04, 5.35543143e-04, 8.94237393e-04, 1.00276429e-03,\n",
       "        1.05517655e-03, 1.42093801e-03, 1.17369672e-03, 3.24843906e-04,\n",
       "        1.18047662e-03, 1.82598769e-03, 9.97325885e-04, 5.63841634e-04,\n",
       "        3.29934019e-04, 8.71348156e-04, 1.64422503e-03, 1.22065522e-03,\n",
       "        3.81791259e-04, 6.54993310e-04, 1.02456851e-03, 5.16547343e-04,\n",
       "        5.09325673e-04, 6.95424183e-04, 5.87892016e-04, 3.16775145e-04,\n",
       "        9.92729127e-04, 3.61176760e-03, 2.27201630e-03, 6.97528481e-04,\n",
       "        5.03913184e-04, 1.21337266e-03, 8.02910485e-04, 6.36440184e-04,\n",
       "        4.78219964e-03, 2.91558929e-03, 9.05174357e-04, 4.10452275e-03,\n",
       "        9.28971227e-04, 1.65466469e-03, 2.48977927e-04, 2.22364987e-03,\n",
       "        8.18557746e-04, 1.32901744e-04, 1.62251045e-03, 7.02040522e-03,\n",
       "        2.18384321e-03, 1.53012072e-03, 1.20703137e-03, 1.13033803e-04,\n",
       "        3.31394161e-04, 1.00042078e-03, 1.21906567e-03, 8.64898873e-04,\n",
       "        1.71509558e-03, 1.28427281e-03, 1.81477758e-04, 2.56658299e-04,\n",
       "        9.82987772e-04, 2.37547399e-04, 1.25721729e-03, 1.26302516e-03,\n",
       "        2.59371039e-04, 7.76012714e-04, 9.90221613e-04, 1.00918541e-03,\n",
       "        2.31025344e-04, 4.66785537e-04, 4.78887950e-04, 1.22133877e-03,\n",
       "        5.36340632e-04, 1.70616480e-04, 1.36584898e-03, 5.51830493e-04,\n",
       "        6.87621989e-04, 1.24247854e-03, 3.48008662e-04, 4.27117728e-04,\n",
       "        1.76750008e-04, 2.24250918e-03, 2.80368304e-04, 9.59162852e-04,\n",
       "        4.87617338e-04, 1.07496790e-03, 9.72026669e-03, 3.64083197e-03,\n",
       "        1.24189593e-03, 1.29406936e-03, 2.00518743e-03, 1.73167378e-04,\n",
       "        2.23277506e-03, 2.25474536e-04, 2.69998092e-04, 6.02361705e-04,\n",
       "        1.01673941e-03, 4.85191835e-04, 5.28606843e-04, 3.18602928e-04,\n",
       "        1.25782016e-03, 1.77058941e-03, 7.41980719e-04, 6.05904145e-04,\n",
       "        1.91434719e-03, 2.45529533e-03, 1.87821907e-03, 1.46942442e-03,\n",
       "        1.16440534e-03, 1.32685926e-03, 6.17882186e-04, 1.89113991e-03,\n",
       "        9.99836345e-04, 2.97215458e-04, 2.08776912e-03, 5.17950135e-04,\n",
       "        6.51032609e-04, 7.25249112e-04, 3.93575683e-04, 2.64730612e-03,\n",
       "        2.99342759e-04, 1.70232411e-03, 1.63124509e-03, 1.13132666e-03,\n",
       "        1.58478225e-03, 5.13479214e-03, 5.88411114e-04, 1.64356882e-03,\n",
       "        2.05133415e-03, 1.17599204e-03, 1.52082870e-03, 2.07205006e-03,\n",
       "        2.51745417e-03, 4.20614458e-03, 9.04242166e-04, 3.95637471e-04,\n",
       "        7.72981661e-04, 1.77340788e-03, 3.94671735e-04, 1.36539416e-03,\n",
       "        6.88856961e-04, 3.65438740e-04, 1.02120341e-03, 1.46194521e-03,\n",
       "        6.51779430e-04, 9.74494766e-04, 1.62798865e-03, 3.42440053e-04,\n",
       "        3.25083013e-03, 3.08423164e-03, 1.32033832e-03, 3.76602365e-04,\n",
       "        4.73874158e-03, 1.47452223e-03, 1.53372892e-03, 5.25854172e-04,\n",
       "        6.83348951e-04, 2.37912195e-03, 1.71700887e-03, 1.52741887e-03,\n",
       "        1.81007446e-03, 7.79478727e-04, 6.82981813e-04, 5.78134298e-04,\n",
       "        1.52295853e-03, 2.42176387e-04, 1.51916924e-03, 2.91506951e-03,\n",
       "        2.89241127e-04, 1.12181158e-03, 1.74083764e-03, 7.29762738e-04,\n",
       "        6.86320046e-04, 5.80058282e-04, 8.17436384e-04, 1.23733567e-03,\n",
       "        1.60050504e-03, 1.37649998e-03, 1.37380926e-03, 7.74597631e-04,\n",
       "        4.40681422e-04, 2.74370584e-04, 1.37808322e-03, 3.81773571e-04,\n",
       "        8.93617198e-04, 1.37384326e-03, 2.32460487e-04, 3.23498692e-04,\n",
       "        6.75258132e-04, 3.58650513e-04, 1.36594207e-03, 1.55836293e-04,\n",
       "        3.04425987e-04, 2.61180472e-03, 3.58556729e-03, 7.76412958e-04,\n",
       "        3.15399626e-04, 3.28173536e-04, 2.69468057e-04, 6.17692168e-04,\n",
       "        3.10298145e-04, 1.56741324e-03, 2.75523390e-03, 5.82472935e-05,\n",
       "        1.00873808e-03, 3.90502471e-04, 9.01505166e-04, 2.51775411e-04,\n",
       "        5.40964525e-04, 1.19170546e-03, 2.79591372e-03, 8.18344166e-04,\n",
       "        2.01839217e-03, 9.96581043e-04, 9.56948565e-04, 3.45037206e-04,\n",
       "        3.87252770e-04, 1.65057009e-03, 9.08192849e-04, 4.82193607e-04,\n",
       "        1.38670067e-04, 1.17591268e-03, 9.45379163e-04, 1.07951395e-03,\n",
       "        1.16781168e-03, 1.25207896e-03, 5.68101669e-04, 1.74099783e-03,\n",
       "        3.02758050e-04, 1.06311597e-03, 1.83403471e-03, 6.85059527e-04,\n",
       "        1.03765287e-03, 1.35549192e-03, 2.52734220e-03, 8.73336887e-04,\n",
       "        1.73897354e-03, 1.53567019e-03, 1.06519516e-03, 9.86864657e-04,\n",
       "        2.19046960e-03, 1.02955056e-03, 1.71360102e-03, 1.59682664e-03,\n",
       "        6.44740347e-04, 9.80074261e-04, 1.01389350e-04, 7.71825502e-04,\n",
       "        9.88942774e-04, 5.18024126e-04, 1.74681809e-03, 9.70718303e-04,\n",
       "        4.57611680e-03, 5.31057069e-04, 3.52396158e-04, 1.81574644e-04,\n",
       "        1.20698411e-03, 2.57558763e-03, 3.38169423e-04, 7.14928530e-04,\n",
       "        5.83827916e-04, 1.86766680e-03, 5.14445434e-04, 1.74297198e-03,\n",
       "        1.88828845e-03, 7.00417585e-04, 7.79285726e-04, 2.70300515e-03,\n",
       "        2.33162430e-03, 1.65306931e-03, 2.94856860e-04, 1.28405620e-03,\n",
       "        1.43684189e-03, 4.26954847e-04, 1.43070164e-03, 2.53908605e-03,\n",
       "        2.76397465e-03, 1.33975159e-03, 3.84757802e-04, 2.47352877e-04,\n",
       "        2.11572255e-04, 5.12412694e-04, 1.95527386e-03, 5.22055911e-04,\n",
       "        5.80951972e-04, 1.76517694e-03, 1.70246589e-03, 1.85168203e-03,\n",
       "        4.15634135e-04, 2.53919172e-03, 9.41501685e-04, 1.51021998e-03,\n",
       "        6.66748169e-04, 7.41585988e-04, 7.29730608e-04, 7.01337928e-04,\n",
       "        1.30702903e-03, 1.19677229e-03, 1.09603107e-03, 1.12088452e-03,\n",
       "        4.68898992e-04, 6.37675642e-04, 1.20692242e-04, 2.24958480e-04,\n",
       "        1.22920370e-03, 2.58071414e-03, 1.65283172e-03, 3.84323810e-04,\n",
       "        1.21304324e-03, 5.90014349e-04, 8.41686022e-04, 1.72303138e-03,\n",
       "        9.10710431e-04, 1.02551305e-03, 1.14262279e-03, 1.13565429e-03,\n",
       "        1.10127724e-03, 1.44849709e-03, 6.03804164e-04, 7.40088377e-04,\n",
       "        1.35721088e-03, 2.58457277e-03, 2.77727522e-03, 8.34532141e-04,\n",
       "        1.34213309e-03, 2.49491682e-04, 2.37142190e-03, 6.20444976e-04,\n",
       "        5.52732505e-04, 1.41644745e-03, 8.93853607e-05, 5.84768514e-04,\n",
       "        6.24997530e-04, 1.23736351e-03, 5.50471282e-04, 2.88674815e-03,\n",
       "        4.01482321e-04, 1.77375658e-03, 1.97254758e-03, 1.73708395e-03,\n",
       "        4.23542499e-04, 1.16601575e-03, 5.74665607e-04, 1.81574055e-04,\n",
       "        1.24238309e-04, 1.48046735e-03, 1.74173294e-03, 9.80503216e-03,\n",
       "        1.94533350e-04, 9.30819105e-04, 1.21496793e-03, 6.83391803e-04,\n",
       "        3.33205447e-04, 1.52861311e-03, 6.34099846e-04, 5.18280965e-04,\n",
       "        5.18232823e-04, 4.21771413e-04, 5.40432548e-04, 5.78355374e-04,\n",
       "        8.68098258e-04, 1.58364595e-03, 1.48359911e-03, 6.29710480e-04,\n",
       "        7.67293418e-04, 1.78427380e-04, 1.39575024e-03, 3.90791066e-04,\n",
       "        1.50590377e-03, 2.05022750e-03, 1.12181234e-03, 1.13973231e-03,\n",
       "        2.70113464e-03, 9.18119414e-04, 1.30237842e-03, 6.52806910e-04,\n",
       "        3.40141171e-04, 1.26162863e-03, 2.47853362e-03, 1.87461698e-03,\n",
       "        1.98650214e-03, 2.58839991e-04, 6.58790155e-04, 1.05132643e-03,\n",
       "        6.18411072e-04, 9.47052221e-04, 3.05430574e-04, 4.00022111e-04,\n",
       "        2.81998504e-03, 1.90759920e-03, 1.14361269e-03, 2.93760026e-03,\n",
       "        1.13589974e-03, 6.30262342e-04, 1.79744008e-03, 9.17393434e-04,\n",
       "        4.10005252e-04, 7.65537526e-04, 3.58470166e-04, 6.91071580e-04,\n",
       "        2.10787624e-03, 6.51197731e-04, 6.50688523e-04, 2.55733281e-03,\n",
       "        1.64043748e-03, 1.57035513e-03, 6.19741569e-04, 3.48286918e-03,\n",
       "        3.67512581e-04, 2.96429419e-03, 7.55145020e-04, 2.53606761e-03,\n",
       "        1.40856927e-03, 7.29055089e-04, 1.66047297e-04, 1.27077872e-03,\n",
       "        1.50457990e-03, 4.53515743e-04, 1.92527950e-03, 1.29762752e-03,\n",
       "        8.75037754e-04, 1.07634157e-04, 1.83438993e-03, 3.68321516e-04,\n",
       "        1.60967121e-03, 8.86496416e-04, 1.28394318e-04, 2.33061587e-03,\n",
       "        1.52880211e-03, 3.22645175e-04, 3.47899600e-04, 3.52494440e-03,\n",
       "        1.03850511e-03, 1.61796529e-03, 2.81936984e-03, 8.99519341e-04,\n",
       "        9.72567680e-04, 8.88247753e-04, 2.83027554e-04, 1.29411590e-03,\n",
       "        2.62870257e-03, 4.61685177e-04, 4.55673447e-04, 1.37391659e-03,\n",
       "        1.03079314e-03, 1.46879720e-03, 7.05294652e-04, 9.26112233e-04,\n",
       "        1.14022662e-03, 1.19076339e-03, 4.81810819e-04, 1.32390555e-04,\n",
       "        7.48064416e-04, 6.87484351e-04, 3.01541011e-03, 2.91188176e-04,\n",
       "        5.80512559e-04, 1.13855991e-03, 1.91580583e-03, 6.67897220e-04,\n",
       "        1.37630362e-03, 5.45190726e-04, 2.16968747e-03, 2.10898884e-03,\n",
       "        2.02753769e-03, 4.87735646e-04, 2.08517384e-04, 1.03909866e-03,\n",
       "        1.17237182e-04, 1.64379593e-03, 1.33295954e-03, 5.34925841e-04,\n",
       "        3.32352373e-04, 1.78664963e-03, 3.87212895e-04, 3.79303426e-04,\n",
       "        4.98863302e-04, 7.52942201e-04, 6.72209883e-04, 2.34577935e-03,\n",
       "        6.69086792e-04, 5.24860034e-04, 9.59882225e-04, 4.46919808e-04,\n",
       "        4.09371701e-04, 2.26855943e-03, 3.63422703e-04, 5.11030714e-04,\n",
       "        1.21203280e-03, 1.46417955e-03, 1.14376406e-03, 5.02857501e-04,\n",
       "        6.10421192e-04, 4.97076759e-04, 9.96123944e-04, 2.24763000e-04,\n",
       "        1.71108939e-03, 2.37494900e-03, 1.04990620e-04, 5.99433310e-04,\n",
       "        1.01143876e-03, 7.32011646e-04, 8.27513000e-04, 2.16814082e-03,\n",
       "        6.77846396e-04, 3.36134029e-04, 3.27448406e-04, 2.71762352e-04,\n",
       "        4.94086998e-04, 7.94473347e-04, 1.41478245e-03, 1.20091097e-03,\n",
       "        2.62092562e-03, 3.69998277e-04, 9.67429146e-04, 3.41490597e-03,\n",
       "        3.61141681e-04, 4.42013900e-04, 1.36473343e-03, 1.08413469e-03,\n",
       "        1.88084797e-03, 8.49733470e-04, 5.10965016e-04, 2.52356092e-03,\n",
       "        1.04394395e-03, 7.71164805e-04, 9.92931293e-04, 1.75945110e-03,\n",
       "        1.85087858e-03, 9.05905913e-04, 5.87777303e-04, 3.25100147e-05,\n",
       "        1.07691929e-03, 4.01968613e-03, 2.39172868e-04, 3.34056452e-04,\n",
       "        4.27958388e-04, 4.71615395e-04, 8.95349103e-04, 2.28445260e-03,\n",
       "        3.80420661e-04, 6.77545316e-04, 1.09027319e-03, 1.50521004e-04,\n",
       "        5.91131169e-04, 3.47401046e-03, 2.30392138e-03, 9.76733446e-04,\n",
       "        9.71733797e-04, 9.13674583e-04, 2.12788514e-03, 9.32713037e-04,\n",
       "        3.59919481e-03, 1.07302379e-03, 2.35893976e-03, 1.31961442e-04,\n",
       "        8.41864847e-04, 2.51626118e-03, 1.93062692e-03, 2.16146240e-03,\n",
       "        1.97525075e-03, 1.00002628e-03, 9.89089291e-04, 4.56573295e-04,\n",
       "        8.49338901e-04, 6.44437655e-04, 1.44582081e-03, 2.45389012e-03,\n",
       "        4.47646044e-04, 6.40106635e-04, 2.89578839e-03, 3.80924777e-04,\n",
       "        1.36837728e-03, 1.48083987e-03, 9.42558789e-04, 7.30137105e-04,\n",
       "        2.03378287e-04, 2.16051540e-03, 1.34812323e-03, 1.37831560e-04,\n",
       "        1.48380763e-03, 3.43481809e-03, 2.00534410e-04, 8.54814638e-04,\n",
       "        1.39170429e-04, 1.94834367e-04, 3.13273697e-03, 1.16806733e-03,\n",
       "        7.60750312e-05, 1.24892113e-03, 4.13075132e-04]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.82352941, 0.85148515, 0.84      , 0.86      , 0.87378641,\n",
       "        0.85714286, 0.85436893, 0.85714286, 0.85714286, 0.81553398,\n",
       "        0.83018868, 0.81904762, 0.84615385, 0.85714286, 0.85714286,\n",
       "        0.83495146, 0.86538462, 0.84615385, 0.83495146, 0.81553398,\n",
       "        0.80392157, 0.85436893, 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.83495146, 0.82692308, 0.83809524, 0.7961165 ,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.84      , 0.86538462,\n",
       "        0.84615385, 0.85148515, 0.83168317, 0.80769231, 0.8627451 ,\n",
       "        0.8627451 , 0.87128713, 0.85436893, 0.8411215 , 0.82692308,\n",
       "        0.83168317, 0.83495146, 0.83809524, 0.84      , 0.8627451 ,\n",
       "        0.86538462, 0.84313725, 0.87378641, 0.86538462, 0.82352941,\n",
       "        0.85436893, 0.83809524, 0.83168317, 0.84615385, 0.85436893,\n",
       "        0.82352941, 0.85436893, 0.85436893, 0.82352941, 0.83495146,\n",
       "        0.83495146, 0.83168317, 0.84313725, 0.84313725, 0.82352941,\n",
       "        0.84615385, 0.86538462, 0.84615385, 0.83495146, 0.82692308,\n",
       "        0.85436893, 0.84313725, 0.84313725, 0.83495146, 0.85714286,\n",
       "        0.83809524, 0.84      , 0.84313725, 0.81553398, 0.8627451 ,\n",
       "        0.85148515, 0.8627451 , 0.85436893, 0.85714286, 0.85436893,\n",
       "        0.80392157, 0.82352941, 0.83809524, 0.85148515, 0.85148515,\n",
       "        0.8627451 , 0.84313725, 0.84313725, 0.86538462, 0.82352941,\n",
       "        0.83495146, 0.85436893, 0.84      , 0.85436893, 0.84615385,\n",
       "        0.82352941, 0.84313725, 0.84615385, 0.80769231, 0.84313725,\n",
       "        0.85148515, 0.82352941, 0.85436893, 0.84615385, 0.81553398,\n",
       "        0.83495146, 0.83809524, 0.80392157, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.84313725, 0.83495146, 0.81553398, 0.85436893,\n",
       "        0.85714286, 0.83168317, 0.84313725, 0.84313725, 0.8627451 ,\n",
       "        0.85148515, 0.84      , 0.82352941, 0.86538462, 0.84615385,\n",
       "        0.82      , 0.82352941, 0.83495146, 0.84536082, 0.84313725,\n",
       "        0.85148515, 0.81632653, 0.84615385, 0.85436893, 0.8       ,\n",
       "        0.83168317, 0.85436893, 0.80808081, 0.84313725, 0.85436893,\n",
       "        0.76767677, 0.85148515, 0.85148515, 0.8       , 0.83168317,\n",
       "        0.8627451 , 0.81632653, 0.84313725, 0.8627451 , 0.7755102 ,\n",
       "        0.82352941, 0.8627451 , 0.82828283, 0.83168317, 0.84313725,\n",
       "        0.82474227, 0.85148515, 0.85436893, 0.79591837, 0.84      ,\n",
       "        0.8627451 , 0.81632653, 0.85148515, 0.85148515, 0.82474227,\n",
       "        0.85148515, 0.8627451 , 0.79591837, 0.84313725, 0.86538462,\n",
       "        0.75555556, 0.8125    , 0.81188119, 0.76404494, 0.82105263,\n",
       "        0.84      , 0.74157303, 0.81632653, 0.84615385, 0.76595745,\n",
       "        0.79591837, 0.84      , 0.77777778, 0.80412371, 0.84848485,\n",
       "        0.71111111, 0.7628866 , 0.82352941, 0.76595745, 0.82      ,\n",
       "        0.83168317, 0.80434783, 0.81632653, 0.84      , 0.75268817,\n",
       "        0.79591837, 0.83168317, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.78723404, 0.81632653, 0.84      , 0.76086957, 0.78350515,\n",
       "        0.8       , 0.75      , 0.82828283, 0.82      , 0.7826087 ,\n",
       "        0.82474227, 0.82828283, 0.76923077, 0.79591837, 0.83168317]),\n",
       " 'split1_test_score': array([0.73469388, 0.72164948, 0.7       , 0.72164948, 0.70833333,\n",
       "        0.72727273, 0.72164948, 0.71428571, 0.73469388, 0.70212766,\n",
       "        0.69387755, 0.70588235, 0.72340426, 0.72727273, 0.72      ,\n",
       "        0.71578947, 0.72916667, 0.75      , 0.69473684, 0.71428571,\n",
       "        0.67307692, 0.69473684, 0.69387755, 0.71287129, 0.72916667,\n",
       "        0.73469388, 0.76      , 0.70833333, 0.71287129, 0.69230769,\n",
       "        0.67368421, 0.74      , 0.7254902 , 0.72164948, 0.74226804,\n",
       "        0.76767677, 0.72164948, 0.70588235, 0.69902913, 0.68817204,\n",
       "        0.73267327, 0.7184466 , 0.70833333, 0.76      , 0.76767677,\n",
       "        0.72164948, 0.70103093, 0.71428571, 0.76767677, 0.72164948,\n",
       "        0.71428571, 0.74226804, 0.72916667, 0.72164948, 0.70212766,\n",
       "        0.70833333, 0.69387755, 0.71578947, 0.72727273, 0.69387755,\n",
       "        0.70833333, 0.70833333, 0.73469388, 0.66666667, 0.70833333,\n",
       "        0.72      , 0.65957447, 0.70833333, 0.72      , 0.71578947,\n",
       "        0.73469388, 0.75510204, 0.66666667, 0.72727273, 0.70588235,\n",
       "        0.69473684, 0.70103093, 0.74      , 0.71578947, 0.73469388,\n",
       "        0.75510204, 0.66666667, 0.72727273, 0.7184466 , 0.69387755,\n",
       "        0.72727273, 0.75247525, 0.71578947, 0.74226804, 0.7755102 ,\n",
       "        0.69387755, 0.71428571, 0.72727273, 0.72164948, 0.70833333,\n",
       "        0.74747475, 0.72164948, 0.73469388, 0.71428571, 0.69473684,\n",
       "        0.72916667, 0.72164948, 0.70833333, 0.70103093, 0.72164948,\n",
       "        0.70833333, 0.70833333, 0.70833333, 0.68041237, 0.70833333,\n",
       "        0.73267327, 0.67368421, 0.6875    , 0.69387755, 0.71578947,\n",
       "        0.70833333, 0.70833333, 0.69473684, 0.72164948, 0.72      ,\n",
       "        0.67368421, 0.68085106, 0.72      , 0.69473684, 0.70103093,\n",
       "        0.74747475, 0.68085106, 0.70833333, 0.72      , 0.68085106,\n",
       "        0.69473684, 0.72727273, 0.69473684, 0.70103093, 0.74747475,\n",
       "        0.70833333, 0.72164948, 0.72164948, 0.68817204, 0.72727273,\n",
       "        0.70833333, 0.70833333, 0.72164948, 0.70833333, 0.68085106,\n",
       "        0.69473684, 0.70833333, 0.68085106, 0.70103093, 0.71578947,\n",
       "        0.70212766, 0.70103093, 0.70833333, 0.69473684, 0.6875    ,\n",
       "        0.70833333, 0.6875    , 0.6875    , 0.71578947, 0.70103093,\n",
       "        0.70833333, 0.6875    , 0.68085106, 0.69473684, 0.69473684,\n",
       "        0.67368421, 0.67368421, 0.70833333, 0.70833333, 0.70103093,\n",
       "        0.6875    , 0.69473684, 0.69473684, 0.74747475, 0.66666667,\n",
       "        0.67368421, 0.6875    , 0.70103093, 0.70103093, 0.69473684,\n",
       "        0.67391304, 0.66666667, 0.70103093, 0.66666667, 0.65957447,\n",
       "        0.71428571, 0.66666667, 0.68041237, 0.6875    , 0.65957447,\n",
       "        0.67368421, 0.6875    , 0.65263158, 0.65957447, 0.70103093,\n",
       "        0.66666667, 0.65979381, 0.70103093, 0.66666667, 0.68041237,\n",
       "        0.69387755, 0.66666667, 0.6875    , 0.71428571, 0.6875    ,\n",
       "        0.67346939, 0.70103093, 0.69473684, 0.68686869, 0.69387755,\n",
       "        0.66666667, 0.66666667, 0.6875    , 0.6875    , 0.68041237,\n",
       "        0.6875    , 0.67368421, 0.68041237, 0.68041237, 0.65979381,\n",
       "        0.67346939, 0.6875    , 0.70103093, 0.67346939, 0.69387755,\n",
       "        0.73469388, 0.72164948, 0.7       , 0.72164948, 0.70833333,\n",
       "        0.72727273, 0.72164948, 0.71428571, 0.73469388, 0.70212766,\n",
       "        0.69387755, 0.70588235, 0.72340426, 0.72727273, 0.72      ,\n",
       "        0.71578947, 0.72916667, 0.75      , 0.69473684, 0.71428571,\n",
       "        0.67307692, 0.69473684, 0.69387755, 0.71287129, 0.72916667,\n",
       "        0.73469388, 0.76      , 0.70833333, 0.71287129, 0.69230769,\n",
       "        0.67368421, 0.74      , 0.7254902 , 0.72164948, 0.74226804,\n",
       "        0.76767677, 0.72164948, 0.70588235, 0.69902913, 0.68817204,\n",
       "        0.73267327, 0.7184466 , 0.70833333, 0.76      , 0.76767677,\n",
       "        0.72164948, 0.70103093, 0.71428571, 0.76767677, 0.72164948,\n",
       "        0.71428571, 0.74226804, 0.72916667, 0.72164948, 0.70212766,\n",
       "        0.70833333, 0.69387755, 0.71578947, 0.72727273, 0.69387755,\n",
       "        0.70833333, 0.70833333, 0.73469388, 0.66666667, 0.70833333,\n",
       "        0.72      , 0.65957447, 0.70833333, 0.72      , 0.71578947,\n",
       "        0.73469388, 0.75510204, 0.66666667, 0.72727273, 0.70588235,\n",
       "        0.69473684, 0.70103093, 0.74      , 0.71578947, 0.73469388,\n",
       "        0.75510204, 0.66666667, 0.72727273, 0.7184466 , 0.69387755,\n",
       "        0.72727273, 0.75247525, 0.71578947, 0.74226804, 0.7755102 ,\n",
       "        0.69387755, 0.71428571, 0.72727273, 0.72164948, 0.70833333,\n",
       "        0.74747475, 0.72164948, 0.73469388, 0.71428571, 0.69473684,\n",
       "        0.72916667, 0.72164948, 0.70833333, 0.70103093, 0.72164948,\n",
       "        0.70833333, 0.70833333, 0.70833333, 0.68041237, 0.70833333,\n",
       "        0.73267327, 0.67368421, 0.6875    , 0.69387755, 0.71578947,\n",
       "        0.70833333, 0.70833333, 0.69473684, 0.72164948, 0.72      ,\n",
       "        0.67368421, 0.68085106, 0.72      , 0.69473684, 0.70103093,\n",
       "        0.74747475, 0.68085106, 0.70833333, 0.72      , 0.68085106,\n",
       "        0.69473684, 0.72727273, 0.69473684, 0.70103093, 0.74747475,\n",
       "        0.70833333, 0.72164948, 0.72164948, 0.68817204, 0.72727273,\n",
       "        0.70833333, 0.70833333, 0.72164948, 0.70833333, 0.68085106,\n",
       "        0.69473684, 0.70833333, 0.68085106, 0.70103093, 0.71578947,\n",
       "        0.70212766, 0.70103093, 0.70833333, 0.69473684, 0.6875    ,\n",
       "        0.70833333, 0.6875    , 0.6875    , 0.71578947, 0.70103093,\n",
       "        0.70833333, 0.6875    , 0.68085106, 0.69473684, 0.69473684,\n",
       "        0.67368421, 0.67368421, 0.70833333, 0.70833333, 0.70103093,\n",
       "        0.6875    , 0.69473684, 0.69473684, 0.74747475, 0.66666667,\n",
       "        0.67368421, 0.6875    , 0.70103093, 0.70103093, 0.69473684,\n",
       "        0.67391304, 0.66666667, 0.70103093, 0.66666667, 0.65957447,\n",
       "        0.71428571, 0.66666667, 0.68041237, 0.6875    , 0.65957447,\n",
       "        0.67368421, 0.6875    , 0.65263158, 0.65957447, 0.70103093,\n",
       "        0.66666667, 0.65979381, 0.70103093, 0.66666667, 0.68041237,\n",
       "        0.69387755, 0.66666667, 0.6875    , 0.71428571, 0.6875    ,\n",
       "        0.67346939, 0.70103093, 0.69473684, 0.68686869, 0.69387755,\n",
       "        0.66666667, 0.66666667, 0.6875    , 0.6875    , 0.68041237,\n",
       "        0.6875    , 0.67368421, 0.68041237, 0.68041237, 0.65979381,\n",
       "        0.67346939, 0.6875    , 0.70103093, 0.67346939, 0.69387755,\n",
       "        0.72      , 0.72727273, 0.71428571, 0.70103093, 0.6875    ,\n",
       "        0.69387755, 0.70103093, 0.71428571, 0.72164948, 0.6875    ,\n",
       "        0.71428571, 0.70588235, 0.70103093, 0.6875    , 0.70707071,\n",
       "        0.68041237, 0.68817204, 0.72916667, 0.6875    , 0.68      ,\n",
       "        0.70588235, 0.6875    , 0.6875    , 0.68686869, 0.70212766,\n",
       "        0.71578947, 0.74226804, 0.68041237, 0.71287129, 0.69902913,\n",
       "        0.65957447, 0.68041237, 0.73076923, 0.70833333, 0.72916667,\n",
       "        0.75510204, 0.69387755, 0.71287129, 0.68627451, 0.67368421,\n",
       "        0.72727273, 0.74509804, 0.6875    , 0.70103093, 0.72727273,\n",
       "        0.71428571, 0.72727273, 0.72727273, 0.69387755, 0.70833333,\n",
       "        0.72164948, 0.6875    , 0.70103093, 0.70103093, 0.70103093,\n",
       "        0.72916667, 0.69387755, 0.69387755, 0.70833333, 0.70833333,\n",
       "        0.6875    , 0.6875    , 0.70833333, 0.66666667, 0.70103093,\n",
       "        0.66666667, 0.69387755, 0.72164948, 0.70103093, 0.6875    ,\n",
       "        0.6875    , 0.69473684, 0.69387755, 0.71428571, 0.70588235,\n",
       "        0.6875    , 0.70103093, 0.7254902 , 0.6875    , 0.68085106,\n",
       "        0.72164948, 0.65306122, 0.7254902 , 0.69306931, 0.67368421,\n",
       "        0.70103093, 0.7254902 , 0.6875    , 0.67368421, 0.70103093,\n",
       "        0.67346939, 0.71428571, 0.74      , 0.67346939, 0.71428571,\n",
       "        0.70833333, 0.6875    , 0.71428571, 0.72164948, 0.68041237,\n",
       "        0.70833333, 0.72164948, 0.66666667, 0.70103093, 0.72164948,\n",
       "        0.70103093, 0.69387755, 0.6875    , 0.66666667, 0.70103093,\n",
       "        0.68041237, 0.68686869, 0.71428571, 0.70103093, 0.6875    ,\n",
       "        0.66666667, 0.6875    , 0.66666667, 0.70103093, 0.70707071,\n",
       "        0.67346939, 0.6875    , 0.70103093, 0.6875    , 0.66666667,\n",
       "        0.70103093, 0.66666667, 0.70103093, 0.71287129, 0.68041237,\n",
       "        0.70103093, 0.69387755, 0.6875    , 0.69387755, 0.71428571,\n",
       "        0.65979381, 0.72727273, 0.71428571, 0.67346939, 0.71428571,\n",
       "        0.70103093, 0.68041237, 0.71428571, 0.71428571, 0.64583333,\n",
       "        0.6875    , 0.70833333, 0.64583333, 0.69387755, 0.70103093,\n",
       "        0.66666667, 0.70103093, 0.70103093, 0.64583333, 0.68041237,\n",
       "        0.70103093, 0.64583333, 0.69387755, 0.71428571, 0.66666667,\n",
       "        0.68041237, 0.68041237, 0.65979381, 0.67346939, 0.70103093,\n",
       "        0.64583333, 0.69387755, 0.66666667, 0.68041237, 0.68041237,\n",
       "        0.69387755, 0.64583333, 0.69387755, 0.71428571, 0.64583333,\n",
       "        0.70103093, 0.69387755, 0.68041237, 0.68041237, 0.69387755,\n",
       "        0.59340659, 0.65979381, 0.65979381, 0.59340659, 0.66666667,\n",
       "        0.6875    , 0.6       , 0.67346939, 0.68041237, 0.61702128,\n",
       "        0.64583333, 0.65979381, 0.60215054, 0.64583333, 0.67346939,\n",
       "        0.62365591, 0.65979381, 0.70103093, 0.61702128, 0.64583333,\n",
       "        0.67346939, 0.63157895, 0.64583333, 0.68686869, 0.65263158,\n",
       "        0.67368421, 0.70103093, 0.63157895, 0.65979381, 0.68      ,\n",
       "        0.63157895, 0.65979381, 0.69387755, 0.65263158, 0.6875    ,\n",
       "        0.70103093, 0.63157895, 0.65979381, 0.68      , 0.63829787,\n",
       "        0.66666667, 0.69387755, 0.65263158, 0.6875    , 0.6875    ]),\n",
       " 'split2_test_score': array([0.77669903, 0.75      , 0.74285714, 0.75      , 0.75      ,\n",
       "        0.75      , 0.76923077, 0.75728155, 0.76923077, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.76470588, 0.75      , 0.75      ,\n",
       "        0.74509804, 0.76470588, 0.74074074, 0.74509804, 0.73584906,\n",
       "        0.74285714, 0.76923077, 0.74285714, 0.75      , 0.75728155,\n",
       "        0.74509804, 0.74074074, 0.73786408, 0.73584906, 0.74285714,\n",
       "        0.75      , 0.75      , 0.74285714, 0.74509804, 0.75      ,\n",
       "        0.73394495, 0.72897196, 0.72897196, 0.72897196, 0.76923077,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75      , 0.73394495,\n",
       "        0.77227723, 0.7254902 , 0.74285714, 0.76470588, 0.76470588,\n",
       "        0.75728155, 0.77669903, 0.75      , 0.75728155, 0.75728155,\n",
       "        0.74285714, 0.73584906, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.75728155, 0.76470588, 0.75728155, 0.74509804, 0.74285714,\n",
       "        0.74285714, 0.75728155, 0.75      , 0.72897196, 0.75728155,\n",
       "        0.75728155, 0.73786408, 0.7254902 , 0.73584906, 0.74285714,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74074074, 0.73076923, 0.72897196, 0.72897196, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73786408, 0.75      , 0.74074074,\n",
       "        0.77227723, 0.73076923, 0.73584906, 0.77227723, 0.73076923,\n",
       "        0.75      , 0.75247525, 0.75728155, 0.75      , 0.77227723,\n",
       "        0.75247525, 0.73076923, 0.75728155, 0.73786408, 0.75      ,\n",
       "        0.75247525, 0.74509804, 0.74509804, 0.74509804, 0.75      ,\n",
       "        0.74285714, 0.73786408, 0.74509804, 0.74285714, 0.75      ,\n",
       "        0.74509804, 0.75728155, 0.71153846, 0.72380952, 0.73584906,\n",
       "        0.73786408, 0.73786408, 0.74285714, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.72380952, 0.72222222, 0.73584906, 0.73786408,\n",
       "        0.75      , 0.74285714, 0.75      , 0.75728155, 0.76190476,\n",
       "        0.76      , 0.76470588, 0.75471698, 0.76      , 0.76470588,\n",
       "        0.75      , 0.74747475, 0.75247525, 0.74509804, 0.75247525,\n",
       "        0.75728155, 0.77669903, 0.76470588, 0.75728155, 0.75728155,\n",
       "        0.73786408, 0.74509804, 0.74509804, 0.73786408, 0.73786408,\n",
       "        0.74285714, 0.73786408, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.74509804, 0.74509804, 0.73786408, 0.73076923, 0.72380952,\n",
       "        0.73786408, 0.74509804, 0.73786408, 0.73786408, 0.74509804,\n",
       "        0.73786408, 0.72380952, 0.71698113, 0.72897196, 0.72380952,\n",
       "        0.73076923, 0.75      , 0.72380952, 0.74509804, 0.75      ,\n",
       "        0.73469388, 0.74747475, 0.76      , 0.73469388, 0.74747475,\n",
       "        0.76      , 0.73469388, 0.76      , 0.77227723, 0.74747475,\n",
       "        0.76      , 0.76      , 0.74747475, 0.76      , 0.76470588,\n",
       "        0.74747475, 0.75      , 0.75728155, 0.74      , 0.74509804,\n",
       "        0.75728155, 0.76      , 0.74509804, 0.73786408, 0.74509804,\n",
       "        0.75      , 0.75      , 0.74      , 0.73786408, 0.72380952,\n",
       "        0.76767677, 0.74509804, 0.73786408, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.74      , 0.73076923, 0.73786408, 0.76      ,\n",
       "        0.73076923, 0.73786408, 0.73076923, 0.75      , 0.75      ,\n",
       "        0.77669903, 0.75      , 0.74285714, 0.75      , 0.75      ,\n",
       "        0.75      , 0.76923077, 0.75728155, 0.76923077, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.76470588, 0.75      , 0.75      ,\n",
       "        0.74509804, 0.76470588, 0.74074074, 0.74509804, 0.73584906,\n",
       "        0.74285714, 0.76923077, 0.74285714, 0.75      , 0.75728155,\n",
       "        0.74509804, 0.74074074, 0.73786408, 0.73584906, 0.74285714,\n",
       "        0.75      , 0.75      , 0.74285714, 0.74509804, 0.75      ,\n",
       "        0.73394495, 0.72897196, 0.72897196, 0.72897196, 0.76923077,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75      , 0.73394495,\n",
       "        0.77227723, 0.7254902 , 0.74285714, 0.76470588, 0.76470588,\n",
       "        0.75728155, 0.77669903, 0.75      , 0.75728155, 0.75728155,\n",
       "        0.74285714, 0.73584906, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.75728155, 0.76470588, 0.75728155, 0.74509804, 0.74285714,\n",
       "        0.74285714, 0.75728155, 0.75      , 0.72897196, 0.75728155,\n",
       "        0.75728155, 0.73786408, 0.7254902 , 0.73584906, 0.74285714,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74074074, 0.73076923, 0.72897196, 0.72897196, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73786408, 0.75      , 0.74074074,\n",
       "        0.77227723, 0.73076923, 0.73584906, 0.77227723, 0.73076923,\n",
       "        0.75      , 0.75247525, 0.75728155, 0.75      , 0.77227723,\n",
       "        0.75247525, 0.73076923, 0.75728155, 0.73786408, 0.75      ,\n",
       "        0.75247525, 0.74509804, 0.74509804, 0.74509804, 0.75      ,\n",
       "        0.74285714, 0.73786408, 0.74509804, 0.74285714, 0.75      ,\n",
       "        0.74509804, 0.75728155, 0.71153846, 0.72380952, 0.73584906,\n",
       "        0.73786408, 0.73786408, 0.74285714, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.72380952, 0.72222222, 0.73584906, 0.73786408,\n",
       "        0.75      , 0.74285714, 0.75      , 0.75728155, 0.76190476,\n",
       "        0.76      , 0.76470588, 0.75471698, 0.76      , 0.76470588,\n",
       "        0.75      , 0.74747475, 0.75247525, 0.74509804, 0.75247525,\n",
       "        0.75728155, 0.77669903, 0.76470588, 0.75728155, 0.75728155,\n",
       "        0.73786408, 0.74509804, 0.74509804, 0.73786408, 0.73786408,\n",
       "        0.74285714, 0.73786408, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.74509804, 0.74509804, 0.73786408, 0.73076923, 0.72380952,\n",
       "        0.73786408, 0.74509804, 0.73786408, 0.73786408, 0.74509804,\n",
       "        0.73786408, 0.72380952, 0.71698113, 0.72897196, 0.72380952,\n",
       "        0.73076923, 0.75      , 0.72380952, 0.74509804, 0.75      ,\n",
       "        0.73469388, 0.74747475, 0.76      , 0.73469388, 0.74747475,\n",
       "        0.76      , 0.73469388, 0.76      , 0.77227723, 0.74747475,\n",
       "        0.76      , 0.76      , 0.74747475, 0.76      , 0.76470588,\n",
       "        0.74747475, 0.75      , 0.75728155, 0.74      , 0.74509804,\n",
       "        0.75728155, 0.76      , 0.74509804, 0.73786408, 0.74509804,\n",
       "        0.75      , 0.75      , 0.74      , 0.73786408, 0.72380952,\n",
       "        0.76767677, 0.74509804, 0.73786408, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.74      , 0.73076923, 0.73786408, 0.76      ,\n",
       "        0.73076923, 0.73786408, 0.73076923, 0.75      , 0.75      ,\n",
       "        0.76923077, 0.71153846, 0.72222222, 0.76190476, 0.75728155,\n",
       "        0.75      , 0.75      , 0.75      , 0.75728155, 0.75728155,\n",
       "        0.7184466 , 0.73584906, 0.74285714, 0.75728155, 0.73584906,\n",
       "        0.76923077, 0.76923077, 0.76923077, 0.73584906, 0.74074074,\n",
       "        0.72222222, 0.75      , 0.74285714, 0.73584906, 0.75      ,\n",
       "        0.77358491, 0.76190476, 0.73786408, 0.71698113, 0.71559633,\n",
       "        0.74285714, 0.74285714, 0.74285714, 0.75      , 0.75      ,\n",
       "        0.74766355, 0.75471698, 0.72897196, 0.72897196, 0.73076923,\n",
       "        0.72897196, 0.72897196, 0.73786408, 0.74285714, 0.73394495,\n",
       "        0.77227723, 0.74074074, 0.74285714, 0.76      , 0.75471698,\n",
       "        0.74285714, 0.75      , 0.75      , 0.75728155, 0.73786408,\n",
       "        0.73076923, 0.74074074, 0.75      , 0.75      , 0.75      ,\n",
       "        0.75      , 0.75      , 0.75728155, 0.74509804, 0.71698113,\n",
       "        0.71698113, 0.74285714, 0.75      , 0.74285714, 0.75728155,\n",
       "        0.75728155, 0.75      , 0.71153846, 0.74766355, 0.72897196,\n",
       "        0.73786408, 0.74285714, 0.74285714, 0.74509804, 0.75728155,\n",
       "        0.75      , 0.7184466 , 0.72897196, 0.72222222, 0.73786408,\n",
       "        0.72897196, 0.72897196, 0.73076923, 0.75      , 0.75      ,\n",
       "        0.76      , 0.76635514, 0.75471698, 0.76      , 0.75471698,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.74285714, 0.76      ,\n",
       "        0.73267327, 0.75728155, 0.74      , 0.75      , 0.75      ,\n",
       "        0.74509804, 0.74509804, 0.76470588, 0.76470588, 0.74766355,\n",
       "        0.73584906, 0.73267327, 0.75      , 0.74285714, 0.73786408,\n",
       "        0.75728155, 0.75      , 0.70588235, 0.74285714, 0.72897196,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.74509804, 0.75728155,\n",
       "        0.74285714, 0.70588235, 0.73394495, 0.74074074, 0.73786408,\n",
       "        0.74285714, 0.73584906, 0.73786408, 0.75      , 0.75      ,\n",
       "        0.74747475, 0.78431373, 0.75471698, 0.74747475, 0.75728155,\n",
       "        0.76190476, 0.74747475, 0.76923077, 0.75728155, 0.75247525,\n",
       "        0.76923077, 0.75471698, 0.74      , 0.75      , 0.75      ,\n",
       "        0.75247525, 0.76470588, 0.76923077, 0.73267327, 0.74766355,\n",
       "        0.74285714, 0.76      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.75728155, 0.75728155, 0.74747475, 0.7184466 , 0.74074074,\n",
       "        0.75247525, 0.73076923, 0.73786408, 0.74509804, 0.75      ,\n",
       "        0.75      , 0.75247525, 0.73786408, 0.74766355, 0.75247525,\n",
       "        0.73786408, 0.74285714, 0.73786408, 0.75      , 0.76190476,\n",
       "        0.71578947, 0.74747475, 0.76      , 0.72340426, 0.74747475,\n",
       "        0.75247525, 0.7628866 , 0.74      , 0.77227723, 0.74747475,\n",
       "        0.76470588, 0.76470588, 0.74226804, 0.75247525, 0.75247525,\n",
       "        0.75510204, 0.75247525, 0.75728155, 0.76767677, 0.76470588,\n",
       "        0.75728155, 0.76      , 0.75247525, 0.73786408, 0.74747475,\n",
       "        0.74509804, 0.75      , 0.75510204, 0.75      , 0.73584906,\n",
       "        0.76      , 0.75247525, 0.74509804, 0.74509804, 0.74509804,\n",
       "        0.75728155, 0.74747475, 0.75728155, 0.73584906, 0.74747475,\n",
       "        0.75247525, 0.73076923, 0.75247525, 0.74509804, 0.75728155]),\n",
       " 'split3_test_score': array([0.8       , 0.78846154, 0.77358491, 0.79245283, 0.78846154,\n",
       "        0.78095238, 0.8       , 0.78846154, 0.7961165 , 0.8       ,\n",
       "        0.77669903, 0.77358491, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.78846154, 0.7961165 , 0.78846154, 0.7961165 , 0.77669903,\n",
       "        0.76190476, 0.8       , 0.76190476, 0.76190476, 0.7961165 ,\n",
       "        0.78846154, 0.78095238, 0.78095238, 0.76923077, 0.76923077,\n",
       "        0.8       , 0.77669903, 0.78095238, 0.7961165 , 0.78846154,\n",
       "        0.78504673, 0.76923077, 0.76923077, 0.76923077, 0.78095238,\n",
       "        0.75728155, 0.76923077, 0.80769231, 0.78846154, 0.79245283,\n",
       "        0.76923077, 0.78095238, 0.78095238, 0.78095238, 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.8       , 0.8       ,\n",
       "        0.77358491, 0.77358491, 0.79245283, 0.76923077, 0.78095238,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.78846154, 0.76190476,\n",
       "        0.77358491, 0.80769231, 0.76923077, 0.77358491, 0.8       ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.75471698, 0.76190476,\n",
       "        0.80769231, 0.77358491, 0.78095238, 0.80769231, 0.78846154,\n",
       "        0.8       , 0.8       , 0.76190476, 0.76923077, 0.8       ,\n",
       "        0.78095238, 0.78504673, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76923077, 0.78095238, 0.78846154, 0.76923077, 0.79245283,\n",
       "        0.78846154, 0.76923077, 0.8       , 0.8       , 0.78095238,\n",
       "        0.78846154, 0.77358491, 0.78095238, 0.78846154, 0.78846154,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.8       , 0.79245283,\n",
       "        0.77358491, 0.7961165 , 0.78846154, 0.76923077, 0.7961165 ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.76190476, 0.75471698,\n",
       "        0.80769231, 0.77358491, 0.76923077, 0.80769231, 0.7961165 ,\n",
       "        0.7961165 , 0.80769231, 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.80769231, 0.7961165 , 0.7961165 ,\n",
       "        0.76190476, 0.78846154, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.77227723, 0.78095238, 0.79245283, 0.77669903,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.78095238, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.78846154, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.80769231, 0.7961165 , 0.76190476,\n",
       "        0.80769231, 0.8       , 0.77358491, 0.7961165 , 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76190476, 0.80769231,\n",
       "        0.78504673, 0.77358491, 0.7961165 , 0.80769231, 0.7961165 ,\n",
       "        0.75510204, 0.75728155, 0.76923077, 0.75510204, 0.75728155,\n",
       "        0.76190476, 0.73267327, 0.75728155, 0.76190476, 0.75247525,\n",
       "        0.75728155, 0.78846154, 0.75728155, 0.75728155, 0.78095238,\n",
       "        0.73786408, 0.75728155, 0.76923077, 0.7961165 , 0.77669903,\n",
       "        0.7961165 , 0.7961165 , 0.78846154, 0.78846154, 0.75471698,\n",
       "        0.78095238, 0.78846154, 0.77227723, 0.77669903, 0.7961165 ,\n",
       "        0.77227723, 0.77669903, 0.8       , 0.76923077, 0.78846154,\n",
       "        0.8       , 0.7961165 , 0.78846154, 0.78846154, 0.78431373,\n",
       "        0.78846154, 0.8       , 0.76923077, 0.78846154, 0.80769231,\n",
       "        0.8       , 0.78846154, 0.77358491, 0.79245283, 0.78846154,\n",
       "        0.78095238, 0.8       , 0.78846154, 0.7961165 , 0.8       ,\n",
       "        0.77669903, 0.77358491, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.78846154, 0.7961165 , 0.78846154, 0.7961165 , 0.77669903,\n",
       "        0.76190476, 0.8       , 0.76190476, 0.76190476, 0.7961165 ,\n",
       "        0.78846154, 0.78095238, 0.78095238, 0.76923077, 0.76923077,\n",
       "        0.8       , 0.77669903, 0.78095238, 0.7961165 , 0.78846154,\n",
       "        0.78504673, 0.76923077, 0.76923077, 0.76923077, 0.78095238,\n",
       "        0.75728155, 0.76923077, 0.80769231, 0.78846154, 0.79245283,\n",
       "        0.76923077, 0.78095238, 0.78095238, 0.78095238, 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.8       , 0.8       ,\n",
       "        0.77358491, 0.77358491, 0.79245283, 0.76923077, 0.78095238,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.78846154, 0.76190476,\n",
       "        0.77358491, 0.80769231, 0.76923077, 0.77358491, 0.8       ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.75471698, 0.76190476,\n",
       "        0.80769231, 0.77358491, 0.78095238, 0.80769231, 0.78846154,\n",
       "        0.8       , 0.8       , 0.76190476, 0.76923077, 0.8       ,\n",
       "        0.78095238, 0.78504673, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76923077, 0.78095238, 0.78846154, 0.76923077, 0.79245283,\n",
       "        0.78846154, 0.76923077, 0.8       , 0.8       , 0.78095238,\n",
       "        0.78846154, 0.77358491, 0.78095238, 0.78846154, 0.78846154,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.8       , 0.79245283,\n",
       "        0.77358491, 0.7961165 , 0.78846154, 0.76923077, 0.7961165 ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.76190476, 0.75471698,\n",
       "        0.80769231, 0.77358491, 0.76923077, 0.80769231, 0.7961165 ,\n",
       "        0.7961165 , 0.80769231, 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.80769231, 0.7961165 , 0.7961165 ,\n",
       "        0.76190476, 0.78846154, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.77227723, 0.78095238, 0.79245283, 0.77669903,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.78095238, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.78846154, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.80769231, 0.7961165 , 0.76190476,\n",
       "        0.80769231, 0.8       , 0.77358491, 0.7961165 , 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76190476, 0.80769231,\n",
       "        0.78504673, 0.77358491, 0.7961165 , 0.80769231, 0.7961165 ,\n",
       "        0.75510204, 0.75728155, 0.76923077, 0.75510204, 0.75728155,\n",
       "        0.76190476, 0.73267327, 0.75728155, 0.76190476, 0.75247525,\n",
       "        0.75728155, 0.78846154, 0.75728155, 0.75728155, 0.78095238,\n",
       "        0.73786408, 0.75728155, 0.76923077, 0.7961165 , 0.77669903,\n",
       "        0.7961165 , 0.7961165 , 0.78846154, 0.78846154, 0.75471698,\n",
       "        0.78095238, 0.78846154, 0.77227723, 0.77669903, 0.7961165 ,\n",
       "        0.77227723, 0.77669903, 0.8       , 0.76923077, 0.78846154,\n",
       "        0.8       , 0.7961165 , 0.78846154, 0.78846154, 0.78431373,\n",
       "        0.78846154, 0.8       , 0.76923077, 0.78846154, 0.80769231,\n",
       "        0.8       , 0.78504673, 0.79245283, 0.8       , 0.79245283,\n",
       "        0.8       , 0.81132075, 0.78846154, 0.81132075, 0.8       ,\n",
       "        0.79245283, 0.78095238, 0.80373832, 0.78846154, 0.78095238,\n",
       "        0.81904762, 0.80769231, 0.80769231, 0.81904762, 0.77358491,\n",
       "        0.77358491, 0.81132075, 0.78095238, 0.77358491, 0.83018868,\n",
       "        0.78846154, 0.7961165 , 0.80769231, 0.76923077, 0.78095238,\n",
       "        0.81904762, 0.78095238, 0.78095238, 0.81904762, 0.80769231,\n",
       "        0.8       , 0.8       , 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.78095238, 0.77358491, 0.81904762, 0.8       , 0.8       ,\n",
       "        0.78846154, 0.8       , 0.78504673, 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.77669903, 0.79245283, 0.78846154, 0.81132075,\n",
       "        0.78095238, 0.78846154, 0.8       , 0.78846154, 0.78095238,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.82242991, 0.78095238,\n",
       "        0.77358491, 0.80769231, 0.7961165 , 0.77358491, 0.81132075,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.76923077, 0.77358491,\n",
       "        0.81904762, 0.78504673, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.78846154, 0.80769231, 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.79245283, 0.77358491, 0.81904762, 0.76923077, 0.78095238,\n",
       "        0.77669903, 0.8       , 0.79245283, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.76470588, 0.79245283, 0.7961165 , 0.76470588,\n",
       "        0.8       , 0.79245283, 0.77669903, 0.79245283, 0.77669903,\n",
       "        0.77669903, 0.81904762, 0.80769231, 0.7961165 , 0.80769231,\n",
       "        0.77358491, 0.78095238, 0.8       , 0.77358491, 0.78095238,\n",
       "        0.81904762, 0.7961165 , 0.80769231, 0.78095238, 0.77358491,\n",
       "        0.80769231, 0.79245283, 0.79245283, 0.80769231, 0.80769231,\n",
       "        0.8       , 0.80769231, 0.76923077, 0.77669903, 0.80769231,\n",
       "        0.79245283, 0.78095238, 0.80769231, 0.81904762, 0.79245283,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.77227723, 0.78095238,\n",
       "        0.79245283, 0.76470588, 0.78095238, 0.79245283, 0.76470588,\n",
       "        0.80373832, 0.79245283, 0.76470588, 0.79245283, 0.79245283,\n",
       "        0.76470588, 0.80373832, 0.81904762, 0.77669903, 0.80373832,\n",
       "        0.79245283, 0.76470588, 0.79245283, 0.79245283, 0.76923077,\n",
       "        0.79245283, 0.80769231, 0.77227723, 0.8       , 0.79245283,\n",
       "        0.77227723, 0.78846154, 0.79245283, 0.76470588, 0.8       ,\n",
       "        0.81904762, 0.78846154, 0.8       , 0.79245283, 0.78846154,\n",
       "        0.8       , 0.78504673, 0.78846154, 0.81132075, 0.81904762,\n",
       "        0.77083333, 0.78      , 0.75728155, 0.75510204, 0.78      ,\n",
       "        0.75728155, 0.7628866 , 0.78      , 0.75728155, 0.76767677,\n",
       "        0.77227723, 0.75728155, 0.75510204, 0.77227723, 0.75728155,\n",
       "        0.74226804, 0.77227723, 0.75728155, 0.78      , 0.77227723,\n",
       "        0.78095238, 0.78      , 0.76470588, 0.78095238, 0.76767677,\n",
       "        0.76      , 0.78095238, 0.78350515, 0.77227723, 0.78846154,\n",
       "        0.75510204, 0.76470588, 0.78095238, 0.75510204, 0.76      ,\n",
       "        0.78095238, 0.79591837, 0.77669903, 0.78846154, 0.7628866 ,\n",
       "        0.76923077, 0.79245283, 0.75510204, 0.77227723, 0.78846154]),\n",
       " 'split4_test_score': array([0.76470588, 0.80769231, 0.8       , 0.74      , 0.76767677,\n",
       "        0.78      , 0.71428571, 0.75510204, 0.7755102 , 0.78787879,\n",
       "        0.80412371, 0.8       , 0.76      , 0.75510204, 0.76767677,\n",
       "        0.73267327, 0.75789474, 0.76767677, 0.82      , 0.82828283,\n",
       "        0.79591837, 0.78431373, 0.78787879, 0.77227723, 0.76      ,\n",
       "        0.75510204, 0.76      , 0.78787879, 0.80808081, 0.76767677,\n",
       "        0.78787879, 0.78787879, 0.78      , 0.76      , 0.75510204,\n",
       "        0.77227723, 0.80392157, 0.80808081, 0.79207921, 0.79207921,\n",
       "        0.78787879, 0.78      , 0.78      , 0.75510204, 0.76767677,\n",
       "        0.74226804, 0.78846154, 0.80808081, 0.74509804, 0.76      ,\n",
       "        0.79207921, 0.74      , 0.74      , 0.7628866 , 0.76767677,\n",
       "        0.8125    , 0.7755102 , 0.74      , 0.76767677, 0.75510204,\n",
       "        0.72727273, 0.74226804, 0.7628866 , 0.78787879, 0.82      ,\n",
       "        0.8       , 0.76767677, 0.78350515, 0.78787879, 0.75510204,\n",
       "        0.7755102 , 0.78      , 0.78787879, 0.82      , 0.8       ,\n",
       "        0.79207921, 0.78787879, 0.7755102 , 0.76      , 0.7628866 ,\n",
       "        0.76767677, 0.8       , 0.83168317, 0.8       , 0.79207921,\n",
       "        0.80392157, 0.79207921, 0.77227723, 0.75510204, 0.76      ,\n",
       "        0.74226804, 0.80392157, 0.80392157, 0.73469388, 0.7961165 ,\n",
       "        0.76      , 0.73469388, 0.73267327, 0.74747475, 0.76      ,\n",
       "        0.77227723, 0.79591837, 0.72727273, 0.7755102 , 0.75510204,\n",
       "        0.74509804, 0.72727273, 0.7628866 , 0.78      , 0.79591837,\n",
       "        0.80808081, 0.74747475, 0.76      , 0.79591837, 0.76      ,\n",
       "        0.74747475, 0.7628866 , 0.8       , 0.83168317, 0.80808081,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.76      , 0.75510204,\n",
       "        0.7628866 , 0.80392157, 0.82352941, 0.82      , 0.81188119,\n",
       "        0.78431373, 0.79207921, 0.77227723, 0.7628866 , 0.7628866 ,\n",
       "        0.74226804, 0.75247525, 0.80392157, 0.74226804, 0.7254902 ,\n",
       "        0.76      , 0.72164948, 0.73267327, 0.72340426, 0.75510204,\n",
       "        0.79207921, 0.78787879, 0.74      , 0.77669903, 0.77227723,\n",
       "        0.74509804, 0.73267327, 0.72727273, 0.76767677, 0.79207921,\n",
       "        0.82828283, 0.75510204, 0.78431373, 0.79591837, 0.76767677,\n",
       "        0.74      , 0.75      , 0.8       , 0.80392157, 0.82352941,\n",
       "        0.76767677, 0.7961165 , 0.80392157, 0.74747475, 0.76      ,\n",
       "        0.74226804, 0.8       , 0.8       , 0.83168317, 0.78787879,\n",
       "        0.81553398, 0.80392157, 0.7755102 , 0.78      , 0.7628866 ,\n",
       "        0.73913043, 0.75      , 0.74226804, 0.73913043, 0.75789474,\n",
       "        0.74226804, 0.7311828 , 0.73684211, 0.73469388, 0.73913043,\n",
       "        0.75      , 0.7628866 , 0.73913043, 0.74226804, 0.75510204,\n",
       "        0.7311828 , 0.75510204, 0.76      , 0.77083333, 0.78350515,\n",
       "        0.7755102 , 0.73684211, 0.7755102 , 0.77227723, 0.70967742,\n",
       "        0.74747475, 0.77227723, 0.77894737, 0.78      , 0.80392157,\n",
       "        0.77083333, 0.7755102 , 0.77227723, 0.7311828 , 0.75510204,\n",
       "        0.76      , 0.78723404, 0.77227723, 0.78846154, 0.77894737,\n",
       "        0.78      , 0.80392157, 0.71578947, 0.75510204, 0.78      ,\n",
       "        0.76470588, 0.80769231, 0.8       , 0.74      , 0.76767677,\n",
       "        0.78      , 0.71428571, 0.75510204, 0.7755102 , 0.78787879,\n",
       "        0.80412371, 0.8       , 0.76      , 0.75510204, 0.76767677,\n",
       "        0.73267327, 0.75789474, 0.76767677, 0.82      , 0.82828283,\n",
       "        0.79591837, 0.78431373, 0.78787879, 0.77227723, 0.76      ,\n",
       "        0.75510204, 0.76      , 0.78787879, 0.80808081, 0.76767677,\n",
       "        0.78787879, 0.78787879, 0.78      , 0.76      , 0.75510204,\n",
       "        0.77227723, 0.80392157, 0.80808081, 0.79207921, 0.79207921,\n",
       "        0.78787879, 0.78      , 0.78      , 0.75510204, 0.76767677,\n",
       "        0.74226804, 0.78846154, 0.80808081, 0.74509804, 0.76      ,\n",
       "        0.79207921, 0.74      , 0.74      , 0.7628866 , 0.76767677,\n",
       "        0.8125    , 0.7755102 , 0.74      , 0.76767677, 0.75510204,\n",
       "        0.72727273, 0.74226804, 0.7628866 , 0.78787879, 0.82      ,\n",
       "        0.8       , 0.76767677, 0.78350515, 0.78787879, 0.75510204,\n",
       "        0.7755102 , 0.78      , 0.78787879, 0.82      , 0.8       ,\n",
       "        0.79207921, 0.78787879, 0.7755102 , 0.76      , 0.7628866 ,\n",
       "        0.76767677, 0.8       , 0.83168317, 0.8       , 0.79207921,\n",
       "        0.80392157, 0.79207921, 0.77227723, 0.75510204, 0.76      ,\n",
       "        0.74226804, 0.80392157, 0.80392157, 0.73469388, 0.7961165 ,\n",
       "        0.76      , 0.73469388, 0.73267327, 0.74747475, 0.76      ,\n",
       "        0.77227723, 0.79591837, 0.72727273, 0.7755102 , 0.75510204,\n",
       "        0.74509804, 0.72727273, 0.7628866 , 0.78      , 0.79591837,\n",
       "        0.80808081, 0.74747475, 0.76      , 0.79591837, 0.76      ,\n",
       "        0.74747475, 0.7628866 , 0.8       , 0.83168317, 0.80808081,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.76      , 0.75510204,\n",
       "        0.7628866 , 0.80392157, 0.82352941, 0.82      , 0.81188119,\n",
       "        0.78431373, 0.79207921, 0.77227723, 0.7628866 , 0.7628866 ,\n",
       "        0.74226804, 0.75247525, 0.80392157, 0.74226804, 0.7254902 ,\n",
       "        0.76      , 0.72164948, 0.73267327, 0.72340426, 0.75510204,\n",
       "        0.79207921, 0.78787879, 0.74      , 0.77669903, 0.77227723,\n",
       "        0.74509804, 0.73267327, 0.72727273, 0.76767677, 0.79207921,\n",
       "        0.82828283, 0.75510204, 0.78431373, 0.79591837, 0.76767677,\n",
       "        0.74      , 0.75      , 0.8       , 0.80392157, 0.82352941,\n",
       "        0.76767677, 0.7961165 , 0.80392157, 0.74747475, 0.76      ,\n",
       "        0.74226804, 0.8       , 0.8       , 0.83168317, 0.78787879,\n",
       "        0.81553398, 0.80392157, 0.7755102 , 0.78      , 0.7628866 ,\n",
       "        0.73913043, 0.75      , 0.74226804, 0.73913043, 0.75789474,\n",
       "        0.74226804, 0.7311828 , 0.73684211, 0.73469388, 0.73913043,\n",
       "        0.75      , 0.7628866 , 0.73913043, 0.74226804, 0.75510204,\n",
       "        0.7311828 , 0.75510204, 0.76      , 0.77083333, 0.78350515,\n",
       "        0.7755102 , 0.73684211, 0.7755102 , 0.77227723, 0.70967742,\n",
       "        0.74747475, 0.77227723, 0.77894737, 0.78      , 0.80392157,\n",
       "        0.77083333, 0.7755102 , 0.77227723, 0.7311828 , 0.75510204,\n",
       "        0.76      , 0.78723404, 0.77227723, 0.78846154, 0.77894737,\n",
       "        0.78      , 0.80392157, 0.71578947, 0.75510204, 0.78      ,\n",
       "        0.76      , 0.80392157, 0.8       , 0.74509804, 0.77227723,\n",
       "        0.76767677, 0.74509804, 0.73267327, 0.74747475, 0.78      ,\n",
       "        0.80412371, 0.81632653, 0.74      , 0.77227723, 0.78      ,\n",
       "        0.74      , 0.74747475, 0.74747475, 0.78431373, 0.80808081,\n",
       "        0.7755102 , 0.75247525, 0.76470588, 0.77227723, 0.74509804,\n",
       "        0.7755102 , 0.77227723, 0.78      , 0.79591837, 0.7755102 ,\n",
       "        0.76470588, 0.77227723, 0.77669903, 0.76      , 0.77227723,\n",
       "        0.78431373, 0.79207921, 0.79591837, 0.78787879, 0.78      ,\n",
       "        0.77227723, 0.77227723, 0.75247525, 0.74747475, 0.77227723,\n",
       "        0.74747475, 0.7961165 , 0.83168317, 0.73267327, 0.77669903,\n",
       "        0.74509804, 0.74      , 0.73786408, 0.75247525, 0.74747475,\n",
       "        0.8       , 0.80808081, 0.76      , 0.77227723, 0.76      ,\n",
       "        0.74747475, 0.7254902 , 0.73469388, 0.76470588, 0.8       ,\n",
       "        0.81632653, 0.74      , 0.76470588, 0.76470588, 0.74747475,\n",
       "        0.75247525, 0.77227723, 0.7961165 , 0.77227723, 0.79591837,\n",
       "        0.78431373, 0.76470588, 0.77227723, 0.76470588, 0.75728155,\n",
       "        0.78      , 0.80769231, 0.81188119, 0.78787879, 0.78431373,\n",
       "        0.76470588, 0.79207921, 0.78      , 0.77227723, 0.78      ,\n",
       "        0.75510204, 0.78846154, 0.7961165 , 0.75510204, 0.78095238,\n",
       "        0.75728155, 0.74226804, 0.7254902 , 0.73267327, 0.73469388,\n",
       "        0.77669903, 0.79591837, 0.72727273, 0.7961165 , 0.76      ,\n",
       "        0.76      , 0.73076923, 0.74      , 0.73267327, 0.77669903,\n",
       "        0.82474227, 0.74747475, 0.74509804, 0.77227723, 0.73469388,\n",
       "        0.75      , 0.74747475, 0.76470588, 0.80392157, 0.8       ,\n",
       "        0.74747475, 0.75728155, 0.78      , 0.74747475, 0.74509804,\n",
       "        0.74      , 0.78846154, 0.77227723, 0.8       , 0.77227723,\n",
       "        0.76470588, 0.77227723, 0.76      , 0.74509804, 0.75247525,\n",
       "        0.73333333, 0.76      , 0.78846154, 0.73913043, 0.74509804,\n",
       "        0.8       , 0.7311828 , 0.73267327, 0.7254902 , 0.74468085,\n",
       "        0.75728155, 0.77669903, 0.73684211, 0.73786408, 0.77669903,\n",
       "        0.72164948, 0.7184466 , 0.73076923, 0.74226804, 0.78095238,\n",
       "        0.79207921, 0.73469388, 0.76190476, 0.77669903, 0.72916667,\n",
       "        0.73786408, 0.75      , 0.74226804, 0.77669903, 0.81188119,\n",
       "        0.73469388, 0.75471698, 0.76923077, 0.72164948, 0.73786408,\n",
       "        0.75247525, 0.74226804, 0.76190476, 0.80392157, 0.75510204,\n",
       "        0.77358491, 0.76923077, 0.74226804, 0.73786408, 0.75247525,\n",
       "        0.68965517, 0.74725275, 0.72916667, 0.6744186 , 0.73913043,\n",
       "        0.72727273, 0.66666667, 0.72916667, 0.72727273, 0.68817204,\n",
       "        0.73469388, 0.74      , 0.70212766, 0.73469388, 0.74      ,\n",
       "        0.65217391, 0.74226804, 0.73267327, 0.69565217, 0.74747475,\n",
       "        0.74      , 0.70212766, 0.74747475, 0.74509804, 0.67391304,\n",
       "        0.74      , 0.75247525, 0.70967742, 0.74747475, 0.75247525,\n",
       "        0.7173913 , 0.73469388, 0.74509804, 0.67391304, 0.74      ,\n",
       "        0.76      , 0.70967742, 0.74747475, 0.77669903, 0.7032967 ,\n",
       "        0.74226804, 0.76470588, 0.68131868, 0.72727273, 0.75247525]),\n",
       " 'mean_test_score': array([0.78155639, 0.78443445, 0.77583743, 0.77111749, 0.77765161,\n",
       "        0.77907359, 0.77358221, 0.77445474, 0.78818719, 0.77290935,\n",
       "        0.77274226, 0.76460172, 0.77840962, 0.770205  , 0.76987138,\n",
       "        0.76563523, 0.78100533, 0.77800326, 0.7837393 , 0.77965078,\n",
       "        0.76237049, 0.78391369, 0.76817744, 0.76640095, 0.78498353,\n",
       "        0.77722012, 0.77532892, 0.77223649, 0.77219668, 0.75890046,\n",
       "        0.77486162, 0.78346458, 0.77285024, 0.77764973, 0.77804011,\n",
       "        0.78041659, 0.77562854, 0.7662427 , 0.75786221, 0.77638391,\n",
       "        0.77501194, 0.76972595, 0.78541872, 0.78100975, 0.77934056,\n",
       "        0.76674167, 0.7700608 , 0.77685426, 0.78138358, 0.78021883,\n",
       "        0.78349853, 0.78053291, 0.77691026, 0.78144045, 0.77175383,\n",
       "        0.77832886, 0.76499511, 0.76973222, 0.77538507, 0.76217687,\n",
       "        0.76646635, 0.77767684, 0.78106949, 0.76962101, 0.77916807,\n",
       "        0.78036533, 0.76874205, 0.7708413 , 0.77296092, 0.77818363,\n",
       "        0.78747771, 0.78807395, 0.77009461, 0.77786478, 0.76911914,\n",
       "        0.78145069, 0.77194414, 0.77873773, 0.78070169, 0.77665362,\n",
       "        0.78133136, 0.7720362 , 0.77859398, 0.76713939, 0.77563795,\n",
       "        0.78187455, 0.78679402, 0.7775984 , 0.7795711 , 0.78173769,\n",
       "        0.75953072, 0.77628281, 0.78197476, 0.76691721, 0.7780834 ,\n",
       "        0.78226418, 0.76530685, 0.77580353, 0.77542902, 0.76396953,\n",
       "        0.78155306, 0.77746132, 0.75714424, 0.77200192, 0.77447118,\n",
       "        0.75889737, 0.77243656, 0.77391547, 0.77079905, 0.78188993,\n",
       "        0.78398824, 0.75902791, 0.7670857 , 0.76348356, 0.77137149,\n",
       "        0.77248145, 0.77473492, 0.77079352, 0.78035841, 0.7740264 ,\n",
       "        0.77016788, 0.76839968, 0.77729137, 0.76704894, 0.77187847,\n",
       "        0.78272414, 0.77355192, 0.77496018, 0.77678153, 0.77853151,\n",
       "        0.76975786, 0.78180617, 0.77193157, 0.77704802, 0.78397355,\n",
       "        0.76015779, 0.77345843, 0.77923837, 0.76072616, 0.76998127,\n",
       "        0.77470625, 0.7572939 , 0.76842386, 0.76308846, 0.75797393,\n",
       "        0.77515615, 0.78332173, 0.75810776, 0.77182023, 0.78161867,\n",
       "        0.76035776, 0.7643879 , 0.77243656, 0.76662578, 0.77167913,\n",
       "        0.78446075, 0.75963064, 0.77636275, 0.78403921, 0.76711869,\n",
       "        0.77277376, 0.76881983, 0.77093806, 0.77765785, 0.77334513,\n",
       "        0.76538347, 0.77773703, 0.77949806, 0.7636143 , 0.77306128,\n",
       "        0.76582665, 0.77158437, 0.7688311 , 0.78430396, 0.76520946,\n",
       "        0.77576411, 0.77555031, 0.76729343, 0.77763804, 0.77162178,\n",
       "        0.74477841, 0.75163153, 0.76420292, 0.74332913, 0.75179204,\n",
       "        0.76538867, 0.73474545, 0.75425414, 0.75693174, 0.73973098,\n",
       "        0.75554009, 0.7694666 , 0.74351419, 0.75117175, 0.77005522,\n",
       "        0.73663766, 0.75178242, 0.76550865, 0.75642543, 0.76448986,\n",
       "        0.77425413, 0.75613558, 0.76901093, 0.77227468, 0.7368453 ,\n",
       "        0.75900675, 0.77035394, 0.75719229, 0.7636333 , 0.77089197,\n",
       "        0.76632413, 0.75845135, 0.76752826, 0.75250247, 0.76342264,\n",
       "        0.76812745, 0.76607362, 0.76004064, 0.76469647, 0.76568315,\n",
       "        0.7601966 , 0.77385713, 0.74902065, 0.76203404, 0.77494142,\n",
       "        0.78155639, 0.78443445, 0.77583743, 0.77111749, 0.77765161,\n",
       "        0.77907359, 0.77358221, 0.77445474, 0.78818719, 0.77290935,\n",
       "        0.77274226, 0.76460172, 0.77840962, 0.770205  , 0.76987138,\n",
       "        0.76563523, 0.78100533, 0.77800326, 0.7837393 , 0.77965078,\n",
       "        0.76237049, 0.78391369, 0.76817744, 0.76640095, 0.78498353,\n",
       "        0.77722012, 0.77532892, 0.77223649, 0.77219668, 0.75890046,\n",
       "        0.77486162, 0.78346458, 0.77285024, 0.77764973, 0.77804011,\n",
       "        0.78041659, 0.77562854, 0.7662427 , 0.75786221, 0.77638391,\n",
       "        0.77501194, 0.76972595, 0.78541872, 0.78100975, 0.77934056,\n",
       "        0.76674167, 0.7700608 , 0.77685426, 0.78138358, 0.78021883,\n",
       "        0.78349853, 0.78053291, 0.77691026, 0.78144045, 0.77175383,\n",
       "        0.77832886, 0.76499511, 0.76973222, 0.77538507, 0.76217687,\n",
       "        0.76646635, 0.77767684, 0.78106949, 0.76962101, 0.77916807,\n",
       "        0.78036533, 0.76874205, 0.7708413 , 0.77296092, 0.77818363,\n",
       "        0.78747771, 0.78807395, 0.77009461, 0.77786478, 0.76911914,\n",
       "        0.78145069, 0.77194414, 0.77873773, 0.78070169, 0.77665362,\n",
       "        0.78133136, 0.7720362 , 0.77859398, 0.76713939, 0.77563795,\n",
       "        0.78187455, 0.78679402, 0.7775984 , 0.7795711 , 0.78173769,\n",
       "        0.75953072, 0.77628281, 0.78197476, 0.76691721, 0.7780834 ,\n",
       "        0.78226418, 0.76530685, 0.77580353, 0.77542902, 0.76396953,\n",
       "        0.78155306, 0.77746132, 0.75714424, 0.77200192, 0.77447118,\n",
       "        0.75889737, 0.77243656, 0.77391547, 0.77079905, 0.78188993,\n",
       "        0.78398824, 0.75902791, 0.7670857 , 0.76348356, 0.77137149,\n",
       "        0.77248145, 0.77473492, 0.77079352, 0.78035841, 0.7740264 ,\n",
       "        0.77016788, 0.76839968, 0.77729137, 0.76704894, 0.77187847,\n",
       "        0.78272414, 0.77355192, 0.77496018, 0.77678153, 0.77853151,\n",
       "        0.76975786, 0.78180617, 0.77193157, 0.77704802, 0.78397355,\n",
       "        0.76015779, 0.77345843, 0.77923837, 0.76072616, 0.76998127,\n",
       "        0.77470625, 0.7572939 , 0.76842386, 0.76308846, 0.75797393,\n",
       "        0.77515615, 0.78332173, 0.75810776, 0.77182023, 0.78161867,\n",
       "        0.76035776, 0.7643879 , 0.77243656, 0.76662578, 0.77167913,\n",
       "        0.78446075, 0.75963064, 0.77636275, 0.78403921, 0.76711869,\n",
       "        0.77277376, 0.76881983, 0.77093806, 0.77765785, 0.77334513,\n",
       "        0.76538347, 0.77773703, 0.77949806, 0.7636143 , 0.77306128,\n",
       "        0.76582665, 0.77158437, 0.7688311 , 0.78430396, 0.76520946,\n",
       "        0.77576411, 0.77555031, 0.76729343, 0.77763804, 0.77162178,\n",
       "        0.74477841, 0.75163153, 0.76420292, 0.74332913, 0.75179204,\n",
       "        0.76538867, 0.73474545, 0.75425414, 0.75693174, 0.73973098,\n",
       "        0.75554009, 0.7694666 , 0.74351419, 0.75117175, 0.77005522,\n",
       "        0.73663766, 0.75178242, 0.76550865, 0.75642543, 0.76448986,\n",
       "        0.77425413, 0.75613558, 0.76901093, 0.77227468, 0.7368453 ,\n",
       "        0.75900675, 0.77035394, 0.75719229, 0.7636333 , 0.77089197,\n",
       "        0.76632413, 0.75845135, 0.76752826, 0.75250247, 0.76342264,\n",
       "        0.76812745, 0.76607362, 0.76004064, 0.76469647, 0.76568315,\n",
       "        0.7601966 , 0.77385713, 0.74902065, 0.76203404, 0.77494142,\n",
       "        0.77455204, 0.77585293, 0.77379215, 0.77360675, 0.7766596 ,\n",
       "        0.77373944, 0.77236373, 0.76851268, 0.77897388, 0.76806311,\n",
       "        0.77189951, 0.77161159, 0.76675605, 0.77253264, 0.772203  ,\n",
       "        0.76872844, 0.7755909 , 0.77994367, 0.77233237, 0.76358809,\n",
       "        0.75622425, 0.77113299, 0.76383053, 0.7575255 , 0.77635666,\n",
       "        0.78154301, 0.7815036 , 0.76657837, 0.76661936, 0.75344091,\n",
       "        0.76978604, 0.76617361, 0.77712934, 0.77547619, 0.78490416,\n",
       "        0.78664663, 0.77843178, 0.76773511, 0.75454447, 0.77097817,\n",
       "        0.77444388, 0.77824385, 0.77025118, 0.76649686, 0.7720836 ,\n",
       "        0.77083648, 0.77981629, 0.784991  , 0.76064997, 0.77668936,\n",
       "        0.77269016, 0.75946726, 0.77102685, 0.77292678, 0.76424398,\n",
       "        0.77905144, 0.77385118, 0.76711214, 0.77304519, 0.77073093,\n",
       "        0.75939314, 0.76501029, 0.772474  , 0.76448598, 0.76678318,\n",
       "        0.76170214, 0.76322203, 0.77512183, 0.76506322, 0.76542129,\n",
       "        0.76790543, 0.77570304, 0.77107573, 0.76768174, 0.76625613,\n",
       "        0.77661887, 0.76735559, 0.77294284, 0.76798954, 0.76973471,\n",
       "        0.77564125, 0.76537849, 0.77574227, 0.75612181, 0.77325988,\n",
       "        0.76772935, 0.77657427, 0.77433716, 0.76446701, 0.77327045,\n",
       "        0.75383841, 0.77852636, 0.78427631, 0.76335112, 0.77877861,\n",
       "        0.77193573, 0.75552224, 0.76652951, 0.7717362 , 0.75266831,\n",
       "        0.77053142, 0.78433423, 0.75012768, 0.77879384, 0.77090047,\n",
       "        0.76127148, 0.76638594, 0.76921041, 0.75357093, 0.77524461,\n",
       "        0.77321475, 0.7542997 , 0.77275054, 0.76718081, 0.75130886,\n",
       "        0.76558946, 0.7638373 , 0.74977376, 0.77437986, 0.77055297,\n",
       "        0.75821777, 0.76464576, 0.77025847, 0.76065981, 0.7662215 ,\n",
       "        0.76820619, 0.76007721, 0.76392423, 0.77468966, 0.77219822,\n",
       "        0.77050639, 0.76459124, 0.76331716, 0.77468156, 0.77107353,\n",
       "        0.74657582, 0.77671548, 0.77467361, 0.75554252, 0.76815099,\n",
       "        0.78137473, 0.74802047, 0.7686592 , 0.76877585, 0.74153906,\n",
       "        0.76988676, 0.77731422, 0.73909243, 0.76346634, 0.77491034,\n",
       "        0.73463481, 0.76788138, 0.77431274, 0.73949473, 0.76888996,\n",
       "        0.77823304, 0.74431192, 0.76973079, 0.77923653, 0.73713447,\n",
       "        0.75830805, 0.77162627, 0.75001933, 0.76005964, 0.77784859,\n",
       "        0.74600439, 0.76386209, 0.76411666, 0.74155683, 0.76165529,\n",
       "        0.7756291 , 0.74907294, 0.76902631, 0.78196176, 0.75332289,\n",
       "        0.77279301, 0.77075146, 0.74898488, 0.76454689, 0.77853796,\n",
       "        0.70504803, 0.74940426, 0.74362464, 0.70207529, 0.7508649 ,\n",
       "        0.75290591, 0.70680258, 0.74779252, 0.75667955, 0.71726046,\n",
       "        0.74268574, 0.75235625, 0.71588521, 0.74188068, 0.75434221,\n",
       "        0.6968622 , 0.73794019, 0.75435934, 0.72526153, 0.75005824,\n",
       "        0.7566773 , 0.73561089, 0.74536315, 0.75815664, 0.71887686,\n",
       "        0.74294012, 0.76322834, 0.73176219, 0.74752532, 0.75535717,\n",
       "        0.73026127, 0.74559907, 0.7610052 , 0.71752285, 0.74322064,\n",
       "        0.75985297, 0.7269299 , 0.75390639, 0.76020192, 0.72691292,\n",
       "        0.7510766 , 0.76201766, 0.72215166, 0.74561327, 0.7634803 ]),\n",
       " 'std_test_score': array([0.03273069, 0.04599057, 0.0547373 , 0.04642303, 0.05482315,\n",
       "        0.04388121, 0.05455261, 0.04758839, 0.04336797, 0.04544344,\n",
       "        0.05196629, 0.03863991, 0.04263694, 0.03580744, 0.03887191,\n",
       "        0.04689809, 0.04361717, 0.03641738, 0.0585227 , 0.05019065,\n",
       "        0.05516163, 0.05667653, 0.05294913, 0.03971985, 0.05312597,\n",
       "        0.04641209, 0.03241223, 0.04700957, 0.04488938, 0.0422178 ,\n",
       "        0.0622573 , 0.04326594, 0.03772502, 0.05009109, 0.04129586,\n",
       "        0.03561877, 0.04925861, 0.04377258, 0.03839692, 0.0524212 ,\n",
       "        0.04382682, 0.04032782, 0.05489659, 0.03767369, 0.03345849,\n",
       "        0.03593443, 0.05351025, 0.04430119, 0.03545541, 0.05066606,\n",
       "        0.04956074, 0.0444541 , 0.05045162, 0.04877281, 0.04350458,\n",
       "        0.05124428, 0.05035532, 0.04439227, 0.04623476, 0.04733755,\n",
       "        0.05361763, 0.05445022, 0.04158462, 0.06333664, 0.05529273,\n",
       "        0.05041946, 0.06389614, 0.04411639, 0.04814579, 0.04997856,\n",
       "        0.04767595, 0.04617896, 0.06782458, 0.04910404, 0.04478427,\n",
       "        0.05642483, 0.05084327, 0.04128784, 0.05029965, 0.04306147,\n",
       "        0.03658008, 0.0672276 , 0.04971728, 0.03900358, 0.05508906,\n",
       "        0.04526825, 0.03775959, 0.04945065, 0.04179275, 0.03756962,\n",
       "        0.04128302, 0.04971162, 0.04665737, 0.03999018, 0.05441136,\n",
       "        0.04403269, 0.04459091, 0.04615469, 0.0526542 , 0.03863129,\n",
       "        0.04637101, 0.0517326 , 0.03698051, 0.05241084, 0.0464639 ,\n",
       "        0.0343877 , 0.06068138, 0.05034438, 0.05621873, 0.05152372,\n",
       "        0.0473631 , 0.05620849, 0.05466779, 0.04257872, 0.04081701,\n",
       "        0.05418446, 0.04655117, 0.05709616, 0.05728324, 0.04880323,\n",
       "        0.05902526, 0.0604705 , 0.04688203, 0.04978578, 0.05222449,\n",
       "        0.04104733, 0.06201488, 0.05562067, 0.04858247, 0.06149663,\n",
       "        0.04915566, 0.04466579, 0.04835385, 0.05475766, 0.03733395,\n",
       "        0.039115  , 0.03963317, 0.03907409, 0.05161929, 0.04603162,\n",
       "        0.05157095, 0.04538594, 0.0474633 , 0.05031648, 0.04646517,\n",
       "        0.04665978, 0.048993  , 0.04818755, 0.04564168, 0.04885164,\n",
       "        0.04634543, 0.05071825, 0.06068138, 0.04854137, 0.05750185,\n",
       "        0.05357197, 0.04918693, 0.0569363 , 0.04843405, 0.04866569,\n",
       "        0.05532952, 0.05932907, 0.05420968, 0.05889785, 0.06208822,\n",
       "        0.05749608, 0.06625892, 0.05713068, 0.04293706, 0.05194328,\n",
       "        0.06049999, 0.05274671, 0.05679524, 0.04832773, 0.06218835,\n",
       "        0.06887819, 0.05799551, 0.04993215, 0.05239078, 0.05272984,\n",
       "        0.04707405, 0.0538766 , 0.0482042 , 0.04929275, 0.0562234 ,\n",
       "        0.04492711, 0.04492886, 0.05020161, 0.04618762, 0.0453435 ,\n",
       "        0.05166539, 0.05187702, 0.05384478, 0.05640146, 0.04751109,\n",
       "        0.04256625, 0.05607714, 0.04431303, 0.05064044, 0.05133231,\n",
       "        0.05047416, 0.0533002 , 0.05282524, 0.04607996, 0.0349273 ,\n",
       "        0.05493688, 0.04561167, 0.03668677, 0.04967747, 0.0533113 ,\n",
       "        0.05950718, 0.05310962, 0.0521797 , 0.04953592, 0.05314409,\n",
       "        0.05203417, 0.05493604, 0.05062006, 0.05098347, 0.06019372,\n",
       "        0.05331866, 0.05424989, 0.04567577, 0.05534024, 0.05087692,\n",
       "        0.03273069, 0.04599057, 0.0547373 , 0.04642303, 0.05482315,\n",
       "        0.04388121, 0.05455261, 0.04758839, 0.04336797, 0.04544344,\n",
       "        0.05196629, 0.03863991, 0.04263694, 0.03580744, 0.03887191,\n",
       "        0.04689809, 0.04361717, 0.03641738, 0.0585227 , 0.05019065,\n",
       "        0.05516163, 0.05667653, 0.05294913, 0.03971985, 0.05312597,\n",
       "        0.04641209, 0.03241223, 0.04700957, 0.04488938, 0.0422178 ,\n",
       "        0.0622573 , 0.04326594, 0.03772502, 0.05009109, 0.04129586,\n",
       "        0.03561877, 0.04925861, 0.04377258, 0.03839692, 0.0524212 ,\n",
       "        0.04382682, 0.04032782, 0.05489659, 0.03767369, 0.03345849,\n",
       "        0.03593443, 0.05351025, 0.04430119, 0.03545541, 0.05066606,\n",
       "        0.04956074, 0.0444541 , 0.05045162, 0.04877281, 0.04350458,\n",
       "        0.05124428, 0.05035532, 0.04439227, 0.04623476, 0.04733755,\n",
       "        0.05361763, 0.05445022, 0.04158462, 0.06333664, 0.05529273,\n",
       "        0.05041946, 0.06389614, 0.04411639, 0.04814579, 0.04997856,\n",
       "        0.04767595, 0.04617896, 0.06782458, 0.04910404, 0.04478427,\n",
       "        0.05642483, 0.05084327, 0.04128784, 0.05029965, 0.04306147,\n",
       "        0.03658008, 0.0672276 , 0.04971728, 0.03900358, 0.05508906,\n",
       "        0.04526825, 0.03775959, 0.04945065, 0.04179275, 0.03756962,\n",
       "        0.04128302, 0.04971162, 0.04665737, 0.03999018, 0.05441136,\n",
       "        0.04403269, 0.04459091, 0.04615469, 0.0526542 , 0.03863129,\n",
       "        0.04637101, 0.0517326 , 0.03698051, 0.05241084, 0.0464639 ,\n",
       "        0.0343877 , 0.06068138, 0.05034438, 0.05621873, 0.05152372,\n",
       "        0.0473631 , 0.05620849, 0.05466779, 0.04257872, 0.04081701,\n",
       "        0.05418446, 0.04655117, 0.05709616, 0.05728324, 0.04880323,\n",
       "        0.05902526, 0.0604705 , 0.04688203, 0.04978578, 0.05222449,\n",
       "        0.04104733, 0.06201488, 0.05562067, 0.04858247, 0.06149663,\n",
       "        0.04915566, 0.04466579, 0.04835385, 0.05475766, 0.03733395,\n",
       "        0.039115  , 0.03963317, 0.03907409, 0.05161929, 0.04603162,\n",
       "        0.05157095, 0.04538594, 0.0474633 , 0.05031648, 0.04646517,\n",
       "        0.04665978, 0.048993  , 0.04818755, 0.04564168, 0.04885164,\n",
       "        0.04634543, 0.05071825, 0.06068138, 0.04854137, 0.05750185,\n",
       "        0.05357197, 0.04918693, 0.0569363 , 0.04843405, 0.04866569,\n",
       "        0.05532952, 0.05932907, 0.05420968, 0.05889785, 0.06208822,\n",
       "        0.05749608, 0.06625892, 0.05713068, 0.04293706, 0.05194328,\n",
       "        0.06049999, 0.05274671, 0.05679524, 0.04832773, 0.06218835,\n",
       "        0.06887819, 0.05799551, 0.04993215, 0.05239078, 0.05272984,\n",
       "        0.04707405, 0.0538766 , 0.0482042 , 0.04929275, 0.0562234 ,\n",
       "        0.04492711, 0.04492886, 0.05020161, 0.04618762, 0.0453435 ,\n",
       "        0.05166539, 0.05187702, 0.05384478, 0.05640146, 0.04751109,\n",
       "        0.04256625, 0.05607714, 0.04431303, 0.05064044, 0.05133231,\n",
       "        0.05047416, 0.0533002 , 0.05282524, 0.04607996, 0.0349273 ,\n",
       "        0.05493688, 0.04561167, 0.03668677, 0.04967747, 0.0533113 ,\n",
       "        0.05950718, 0.05310962, 0.0521797 , 0.04953592, 0.05314409,\n",
       "        0.05203417, 0.05493604, 0.05062006, 0.05098347, 0.06019372,\n",
       "        0.05331866, 0.05424989, 0.04567577, 0.05534024, 0.05087692,\n",
       "        0.03539035, 0.05116443, 0.04820586, 0.05362337, 0.0600453 ,\n",
       "        0.05408139, 0.05398968, 0.05063795, 0.04879661, 0.04477185,\n",
       "        0.04697878, 0.04457835, 0.05153355, 0.05454508, 0.05082509,\n",
       "        0.05575448, 0.05927299, 0.0421926 , 0.05435914, 0.04957194,\n",
       "        0.03643849, 0.05715325, 0.0506946 , 0.04409704, 0.05686039,\n",
       "        0.04421478, 0.03187124, 0.05246155, 0.04759593, 0.03862123,\n",
       "        0.06923244, 0.0564495 , 0.04313838, 0.04787171, 0.04791829,\n",
       "        0.03532593, 0.05234509, 0.04335553, 0.04313881, 0.0647045 ,\n",
       "        0.0492686 , 0.04947873, 0.05944671, 0.04878066, 0.03807703,\n",
       "        0.03939238, 0.04000173, 0.04495972, 0.04857093, 0.05015794,\n",
       "        0.05118102, 0.05088318, 0.05909302, 0.05407741, 0.04626874,\n",
       "        0.04676504, 0.05099904, 0.04681398, 0.04539126, 0.04803786,\n",
       "        0.04551337, 0.05933248, 0.05238183, 0.05794512, 0.05050671,\n",
       "        0.06248551, 0.04985415, 0.04163115, 0.04639299, 0.04887407,\n",
       "        0.0524112 , 0.05601322, 0.05849578, 0.03949834, 0.04392981,\n",
       "        0.05890431, 0.04703614, 0.04037017, 0.05113429, 0.05752677,\n",
       "        0.03913168, 0.06923447, 0.04605049, 0.04403761, 0.06400556,\n",
       "        0.05212723, 0.05010053, 0.05979136, 0.05852062, 0.04988251,\n",
       "        0.04365701, 0.03704079, 0.03446061, 0.0567517 , 0.04520231,\n",
       "        0.05220157, 0.05060508, 0.04698921, 0.05335357, 0.04639818,\n",
       "        0.04551531, 0.04419545, 0.0572254 , 0.05113588, 0.04166054,\n",
       "        0.04001545, 0.05593426, 0.05468323, 0.0506997 , 0.04885236,\n",
       "        0.06130245, 0.04596595, 0.0492026 , 0.04747878, 0.04366233,\n",
       "        0.05960979, 0.05068092, 0.05539451, 0.04902497, 0.04879298,\n",
       "        0.05638443, 0.05179689, 0.04542236, 0.04688625, 0.06313016,\n",
       "        0.0545367 , 0.0630284 , 0.04738557, 0.04539552, 0.06174471,\n",
       "        0.05032727, 0.04866165, 0.04899342, 0.0603474 , 0.04496437,\n",
       "        0.05245753, 0.03197261, 0.03976912, 0.05552985, 0.04321543,\n",
       "        0.04943542, 0.04426913, 0.04562528, 0.05071827, 0.05146541,\n",
       "        0.04880906, 0.04782464, 0.05313867, 0.05072887, 0.05040496,\n",
       "        0.03768981, 0.05513288, 0.05522477, 0.05267206, 0.0521186 ,\n",
       "        0.0543099 , 0.05592615, 0.04873345, 0.04945336, 0.03897559,\n",
       "        0.04879844, 0.06098662, 0.05445694, 0.0569764 , 0.05081743,\n",
       "        0.05847043, 0.05359018, 0.06190358, 0.03908591, 0.05460859,\n",
       "        0.05890778, 0.05794964, 0.05371772, 0.04729104, 0.05982197,\n",
       "        0.05158125, 0.05542055, 0.0415503 , 0.05721266, 0.05883313,\n",
       "        0.06276121, 0.05090296, 0.04969962, 0.06273879, 0.05101707,\n",
       "        0.05007935, 0.06405857, 0.04830281, 0.05464566, 0.05782756,\n",
       "        0.05222741, 0.05758243, 0.06195087, 0.0532675 , 0.05589468,\n",
       "        0.0510153 , 0.04034404, 0.04028842, 0.06171576, 0.05739267,\n",
       "        0.05180196, 0.06202219, 0.05544577, 0.05075878, 0.04637382,\n",
       "        0.03976787, 0.04280033, 0.05652069, 0.0489542 , 0.04762808,\n",
       "        0.05413167, 0.0507862 , 0.04827062, 0.04508706, 0.03169567,\n",
       "        0.03323744, 0.05496032, 0.05470154, 0.04831559, 0.05143786,\n",
       "        0.05091324, 0.04678491, 0.0463078 , 0.03729708, 0.04734603]),\n",
       " 'rank_test_score': array([ 57,  18, 182, 323, 143, 105, 247, 229,   1, 265, 273, 479, 120,\n",
       "        350, 363, 457,  76, 132,  32,  92, 516,  30, 400, 441,  13, 156,\n",
       "        204, 288, 292, 553, 217,  36, 267, 145, 130,  82, 194, 447, 566,\n",
       "        174, 210, 372,  10,  74,  98, 431, 356, 163,  67,  88,  34,  80,\n",
       "        161,  65, 309, 122, 474, 369, 202, 518, 439, 139,  72, 374, 103,\n",
       "         84, 390, 334, 261, 126,   5,   3, 354, 134, 379,  63, 299, 111,\n",
       "         78, 170,  70, 295, 113, 418, 191,  49,   7, 149,  94,  53, 545,\n",
       "        179,  44, 427, 128,  42, 468, 184, 200, 493,  59, 151, 573, 297,\n",
       "        227, 555, 280, 239, 337,  47,  26, 549, 423, 504, 320, 277, 219,\n",
       "        339,  86, 237, 352, 397, 154, 425, 305,  40, 249, 212, 165, 116,\n",
       "        366,  51, 302, 159,  28, 536, 251, 100, 527, 360, 221, 569, 395,\n",
       "        514, 564, 207,  38, 562, 307,  55, 531, 487, 280, 433, 312,  16,\n",
       "        543, 176,  24, 420, 270, 387, 329, 141, 253, 465, 137,  96, 501,\n",
       "        258, 453, 318, 385,  21, 470, 186, 197, 415, 147, 315, 634, 610,\n",
       "        490, 640, 606, 463, 659, 594, 575, 648, 586, 376, 638, 613, 358,\n",
       "        656, 608, 460, 579, 483, 235, 582, 382, 286, 654, 551, 346, 571,\n",
       "        499, 332, 444, 557, 412, 603, 508, 403, 451, 540, 476, 455, 534,\n",
       "        241, 623, 520, 214,  57,  18, 182, 323, 143, 105, 247, 229,   1,\n",
       "        265, 273, 479, 120, 350, 363, 457,  76, 132,  32,  92, 516,  30,\n",
       "        400, 441,  13, 156, 204, 288, 292, 553, 217,  36, 267, 145, 130,\n",
       "         82, 194, 447, 566, 174, 210, 372,  10,  74,  98, 431, 356, 163,\n",
       "         67,  88,  34,  80, 161,  65, 309, 122, 474, 369, 202, 518, 439,\n",
       "        139,  72, 374, 103,  84, 390, 334, 261, 126,   5,   3, 354, 134,\n",
       "        379,  63, 299, 111,  78, 170,  70, 295, 113, 418, 191,  49,   7,\n",
       "        149,  94,  53, 545, 179,  44, 427, 128,  42, 468, 184, 200, 493,\n",
       "         59, 151, 573, 297, 227, 555, 280, 239, 337,  47,  26, 549, 423,\n",
       "        504, 320, 277, 219, 339,  86, 237, 352, 397, 154, 425, 305,  40,\n",
       "        249, 212, 165, 116, 366,  51, 302, 159,  28, 536, 251, 100, 527,\n",
       "        360, 221, 569, 395, 514, 564, 207,  38, 562, 307,  55, 531, 487,\n",
       "        280, 433, 312,  16, 543, 176,  24, 420, 270, 387, 329, 141, 253,\n",
       "        465, 137,  96, 501, 258, 453, 318, 385,  21, 470, 186, 197, 415,\n",
       "        147, 315, 634, 610, 490, 640, 606, 463, 659, 594, 575, 648, 586,\n",
       "        376, 638, 613, 358, 656, 608, 460, 579, 483, 235, 582, 382, 286,\n",
       "        654, 551, 346, 571, 499, 332, 444, 557, 412, 603, 508, 403, 451,\n",
       "        540, 476, 455, 534, 241, 623, 520, 214, 226, 181, 244, 246, 169,\n",
       "        245, 284, 394, 108, 405, 304, 317, 430, 276, 290, 392, 196,  90,\n",
       "        285, 503, 581, 322, 498, 568, 178,  61,  62, 436, 435, 599, 365,\n",
       "        450, 158, 199,  15,   9, 119, 409, 590, 328, 231, 124, 349, 438,\n",
       "        294, 336,  91,  12, 530, 168, 275, 547, 327, 264, 489, 107, 243,\n",
       "        422, 260, 342, 548, 473, 279, 485, 429, 523, 513, 209, 472, 462,\n",
       "        407, 189, 325, 411, 446, 172, 414, 263, 406, 368, 190, 467, 188,\n",
       "        584, 256, 410, 173, 233, 486, 255, 597, 118,  23, 510, 110, 301,\n",
       "        588, 437, 311, 602, 344,  20, 617, 109, 331, 525, 443, 378, 598,\n",
       "        206, 257, 593, 272, 417, 612, 459, 497, 620, 232, 343, 560, 478,\n",
       "        348, 529, 449, 399, 538, 495, 223, 291, 345, 481, 511, 224, 326,\n",
       "        629, 167, 225, 585, 402,  69, 626, 393, 389, 647, 362, 153, 651,\n",
       "        507, 216, 661, 408, 234, 650, 384, 125, 636, 371, 102, 653, 559,\n",
       "        314, 619, 539, 136, 630, 496, 492, 646, 524, 193, 622, 381,  46,\n",
       "        600, 269, 341, 625, 482, 115, 673, 621, 637, 674, 616, 601, 672,\n",
       "        627, 577, 670, 644, 605, 671, 645, 592, 675, 652, 591, 666, 618,\n",
       "        578, 658, 633, 561, 668, 643, 512, 662, 628, 589, 663, 632, 526,\n",
       "        669, 642, 542, 664, 596, 533, 665, 615, 522, 667, 631, 506],\n",
       "       dtype=int32)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 3,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7881871942204813"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.793722\n",
      "F1: 0.680556\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=3, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=500, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
